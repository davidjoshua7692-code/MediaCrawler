import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import re
from collections import Counter
import jieba
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

def extract_locations(text):
    """æå–ä¸Šæµ·åŒºåŸŸä¿¡æ¯"""
    shanghai_districts = ['å¾æ±‡', 'é™å®‰', 'é»„æµ¦', 'é•¿å®', 'æ™®é™€', 'è™¹å£', 'æ¨æµ¦', 'æµ¦ä¸œ', 'é—µè¡Œ', 'å®å±±', 'å˜‰å®š', 'æ¾æ±Ÿ', 'é’æµ¦', 'å¥‰è´¤', 'é‡‘å±±', 'å´‡æ˜']
    locations = []

    if pd.isna(text):
        return locations

    for district in shanghai_districts:
        if district in str(text):
            locations.append(district)

    # æå–å…·ä½“åœ°ç‚¹å…³é”®è¯
    location_patterns = [
        r'(\w+è·¯)', r'(\w+å¹¿åœº)', r'(\w+å•†åœº)', r'(\w+è´­ç‰©ä¸­å¿ƒ)',
        r'(\w+å¤§å­¦)', r'(\w+å…¬å›­)', r'å›¾ä¹¦é¦†', r'åœ°é“ç«™'
    ]

    for pattern in location_patterns:
        matches = re.findall(pattern, str(text))
        locations.extend(matches)

    return locations

def analyze_cafe_features(text):
    """åˆ†æå’–å•¡å…ç‰¹å¾"""
    features = {
        'æ’åº§': ['æ’åº§', 'ç”µæº', 'å……ç”µ', 'plug'],
        'å®‰é™': ['å®‰é™', 'æ¸…å‡€', 'ä¸åµ', 'silent', 'quiet'],
        'ç½‘ç»œ': ['wifi', 'wi-fi', 'ç½‘é€Ÿ', 'ç½‘ç»œ'],
        'åœè½¦ä½': ['åœè½¦', 'parking', 'åœè½¦åˆ¸'],
        'å® ç‰©å‹å¥½': ['å® ç‰©', 'ç‹—', 'çŒ«', 'pet', 'å® ç‰©å‹å¥½'],
        'æœ‰å•æ‰€': ['å•æ‰€', 'å«ç”Ÿé—´', 'æ´—æ‰‹é—´', 'wc'],
        'è¥ä¸šæ—¶é—´': ['è¥ä¸š', 'å¼€é—¨', 'å…³é—¨', '24å°æ—¶'],
        'ä»·æ ¼': ['ä»·æ ¼', 'ä¾¿å®œ', 'è´µ', 'å®æƒ ', 'äººå‡'],
    }

    found_features = []
    if pd.isna(text):
        return found_features

    text_lower = str(text).lower()

    for feature, keywords in features.items():
        for keyword in keywords:
            if keyword in text_lower:
                found_features.append(feature)
                break

    return found_features

def analyze_xiaohongshu_data(contents_file, comments_file):
    """ç»¼åˆåˆ†æå°çº¢ä¹¦æ•°æ®"""

    print("=" * 80)
    print("ğŸ“Š å°çº¢ä¹¦ä¸Šæµ·é€‚åˆåŠå…¬çš„å’–å•¡å…æ•°æ®åˆ†ææŠ¥å‘Š")
    print("=" * 80)

    # è¯»å–æ•°æ®
    df_contents = pd.read_csv(contents_file)
    df_comments = pd.read_csv(comments_file)

    print(f"\nâœ… æ•°æ®åŠ è½½æˆåŠŸ!")
    print(f"   å¸–å­æ•°æ®: {len(df_contents)} æ¡")
    print(f"   è¯„è®ºæ•°æ®: {len(df_comments)} æ¡")

    # 1. åŸºç¡€ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“ˆ ä¸€ã€åŸºç¡€æ•°æ®ç»Ÿè®¡")
    print("=" * 80)

    print(f"\nå¸–å­äº’åŠ¨æ•°æ®:")
    print(f"  å¹³å‡ç‚¹èµæ•°: {df_contents['liked_count'].mean():.1f}")
    print(f"  å¹³å‡æ”¶è—æ•°: {df_contents['collected_count'].mean():.1f}")
    print(f"  å¹³å‡è¯„è®ºæ•°: {df_contents['comment_count'].mean():.1f}")
    print(f"  æœ€é«˜ç‚¹èµ: {df_contents['liked_count'].max()}")
    print(f"  æœ€é«˜æ”¶è—: {df_contents['collected_count'].max()}")

    # 2. æå–åœ°ç†ä½ç½®ä¿¡æ¯
    print("\n" + "=" * 80)
    print("ğŸ“ äºŒã€åœ°ç†ä½ç½®åˆ†æ")
    print("=" * 80)

    all_locations = []
    for idx, row in df_contents.iterrows():
        text = f"{row.get('title', '')} {row.get('desc', '')} {row.get('ip_location', '')}"
        locations = extract_locations(text)
        all_locations.extend(locations)

    location_counter = Counter(all_locations)

    if location_counter:
        print(f"\næåŠæœ€å¤šçš„ä¸Šæµ·åŒºåŸŸ (Top 10):")
        for location, count in location_counter.most_common(10):
            print(f"  {location}: {count} æ¬¡")

    # 3. å’–å•¡å…ç‰¹å¾åˆ†æ
    print("\n" + "=" * 80)
    print("â˜• ä¸‰ã€å’–å•¡å…ç‰¹å¾åˆ†æ")
    print("=" * 80)

    all_features = []
    for idx, row in df_contents.iterrows():
        text = f"{row.get('title', '')} {row.get('desc', '')}"
        features = analyze_cafe_features(text)
        all_features.extend(features)

    feature_counter = Counter(all_features)

    if feature_counter:
        print(f"\nç”¨æˆ·æœ€å…³å¿ƒçš„ç‰¹å¾ (Top 10):")
        for feature, count in feature_counter.most_common(10):
            print(f"  {feature}: {count} æ¬¡æåŠ")

    # 4. è¯„è®ºæƒ…æ„Ÿåˆ†æï¼ˆç®€å•å…³é”®è¯ï¼‰
    print("\n" + "=" * 80)
    print("ğŸ’¬ å››ã€è¯„è®ºçƒ­ç‚¹åˆ†æ")
    print("=" * 80)

    positive_words = ['æ¨è', 'å¥½', 'ä¸é”™', 'èˆ’æœ', 'å®‰é™', 'æ£’', 'å–œæ¬¢', 'é€‚åˆ', 'æ–¹ä¾¿']
    negative_words = ['åµ', 'è´µ', 'å·®', 'ä¸å¥½', 'å¤±æœ›', 'æ…¢', 'æŒ¤']

    positive_count = 0
    negative_count = 0

    for idx, row in df_comments.head(100).iterrows():
        comment = str(row.get('content', ''))
        for word in positive_words:
            if word in comment:
                positive_count += 1
                break
        for word in negative_words:
            if word in comment:
                negative_count += 1
                break

    print(f"\nè¯„è®ºæƒ…æ„Ÿå€¾å‘ (åŸºäºå‰100æ¡è¯„è®º):")
    print(f"  ç§¯æè¯„ä»·: {positive_count} æ¡")
    print(f"  æ¶ˆæè¯„ä»·: {negative_count} æ¡")
    print(f"  ç§¯æå æ¯”: {positive_count/(positive_count+negative_count)*100:.1f}%")

    # 5. åˆ›å»ºå¯è§†åŒ–
    print("\n" + "=" * 80)
    print("ğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("=" * 80)

    fig = plt.figure(figsize=(16, 12))

    # å›¾1: äº’åŠ¨æ•°æ®åˆ†å¸ƒ
    ax1 = plt.subplot(2, 3, 1)
    engagement_metrics = ['liked_count', 'collected_count', 'comment_count']
    for i, metric in enumerate(engagement_metrics):
        data = df_contents[metric].dropna()
        if len(data) > 0:
            plt.hist(data, bins=20, alpha=0.5, label=metric.replace('_', ' ').title())
    plt.xlabel('æ•°é‡')
    plt.ylabel('å¸–å­æ•°')
    plt.title('äº’åŠ¨æ•°æ®åˆ†å¸ƒ')
    plt.legend()
    plt.grid(True, alpha=0.3)

    # å›¾2: äº’åŠ¨æ•°æ®ç›¸å…³æ€§
    ax2 = plt.subplot(2, 3, 2)
    corr_data = df_contents[engagement_metrics].dropna()
    if len(corr_data) > 0:
        corr = corr_data.corr()
        sns.heatmap(corr, annot=True, cmap='coolwarm', center=0,
                   square=True, linewidths=1, ax=ax2, cbar_kws={'shrink': 0.8})
        ax2.set_title('äº’åŠ¨æ•°æ®ç›¸å…³æ€§çƒ­å›¾')

    # å›¾3: çƒ­é—¨åŒºåŸŸTop 10
    ax3 = plt.subplot(2, 3, 3)
    if location_counter:
        top_locations = dict(location_counter.most_common(10))
        plt.barh(range(len(top_locations)), list(top_locations.values()))
        plt.yticks(range(len(top_locations)), list(top_locations.keys()))
        plt.xlabel('æåŠæ¬¡æ•°')
        plt.title('çƒ­é—¨ä¸Šæµ·åŒºåŸŸ Top 10')
        plt.grid(True, alpha=0.3, axis='x')

    # å›¾4: å’–å•¡å…ç‰¹å¾æ’å
    ax4 = plt.subplot(2, 3, 4)
    if feature_counter:
        top_features = dict(feature_counter.most_common(10))
        plt.barh(range(len(top_features)), list(top_features.values()))
        plt.yticks(range(len(top_features)), list(top_features.keys()))
        plt.xlabel('æåŠæ¬¡æ•°')
        plt.title('ç”¨æˆ·æœ€å…³å¿ƒçš„å’–å•¡å…ç‰¹å¾')
        plt.grid(True, alpha=0.3, axis='x')

    # å›¾5: IPåœ°ç‚¹åˆ†å¸ƒ
    ax5 = plt.subplot(2, 3, 5)
    ip_locations = df_contents['ip_location'].dropna().value_counts().head(10)
    if len(ip_locations) > 0:
        plt.barh(range(len(ip_locations)), ip_locations.values)
        plt.yticks(range(len(ip_locations)), ip_locations.index)
        plt.xlabel('å¸–å­æ•°')
        plt.title('IPåœ°ç‚¹åˆ†å¸ƒ Top 10')
        plt.grid(True, alpha=0.3, axis='x')

    # å›¾6: æ•°æ®è´¨é‡æ¦‚è§ˆ
    ax6 = plt.subplot(2, 3, 6)
    missing_data = df_contents.isnull().sum()
    missing_data = missing_data[missing_data > 0].sort_values(ascending=True)
    if len(missing_data) > 0:
        plt.barh(range(len(missing_data)), missing_data.values)
        plt.yticks(range(len(missing_data)), missing_data.index)
        plt.xlabel('ç¼ºå¤±æ•°é‡')
        plt.title('æ•°æ®ç¼ºå¤±æƒ…å†µ')
        plt.grid(True, alpha=0.3, axis='x')
    else:
        ax6.text(0.5, 0.5, 'æ•°æ®å®Œæ•´\næ— ç¼ºå¤±', ha='center', va='center',
                fontsize=14, transform=ax6.transAxes)
        ax6.set_title('æ•°æ®è´¨é‡')

    plt.tight_layout()
    plt.savefig('d:/MediaCrawler-main/xiaohongshu_analysis.png', dpi=150, bbox_inches='tight')
    print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜: d:/MediaCrawler-main/xiaohongshu_analysis.png")
    plt.close()

    # 6. å…³é”®æ´å¯Ÿ
    print("\n" + "=" * 80)
    print("ğŸ’¡ äº”ã€å…³é”®æ´å¯Ÿæ€»ç»“")
    print("=" * 80)

    insights = []

    # æ´å¯Ÿ1: æœ€å—æ¬¢è¿çš„å†…å®¹
    if len(df_contents) > 0:
        top_liked = df_contents.nlargest(3, 'liked_count')[['title', 'liked_count']]
        print("\nğŸ”¥ æœ€å—æ¬¢è¿çš„å¸–å­ Top 3:")
        for idx, row in top_liked.iterrows():
            print(f"\n  {row['title'][:50]}...")
            print(f"  ğŸ‘ {row['liked_count']} ä¸ªèµ")

    # æ´å¯Ÿ2: ç”¨æˆ·æœ€å…³å¿ƒçš„ç‰¹å¾
    if feature_counter:
        print(f"\nğŸ¯ ç”¨æˆ·æœ€åœ¨æ„çš„3ä¸ªç‰¹å¾:")
        for feature, count in feature_counter.most_common(3):
            print(f"  â€¢ {feature}: {count} æ¬¡æåŠ")

    # æ´å¯Ÿ3: çƒ­é—¨åŒºåŸŸæ¨è
    if location_counter:
        print(f"\nğŸ™ï¸ æœ€çƒ­é—¨çš„3ä¸ªåŒºåŸŸ:")
        for location, count in location_counter.most_common(3):
            print(f"  â€¢ {location}: {count} æ¬¡æåŠ")

    # æ´å¯Ÿ4: äº’åŠ¨æ¨¡å¼
    if len(df_contents) > 0:
        avg_likes = df_contents['liked_count'].mean()
        avg_collects = df_contents['collected_count'].mean()
        print(f"\nğŸ“Š äº’åŠ¨æ¨¡å¼:")
        print(f"  â€¢ å¹³å‡æ”¶è—æ•°æ˜¯ç‚¹èµæ•°çš„ {avg_collects/avg_likes:.2f} å€")
        print(f"  â€¢ è¯´æ˜ç”¨æˆ·æ›´å€¾å‘æ”¶è—å®ç”¨ä¿¡æ¯")

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆ!")
    print("=" * 80)

    return {
        'contents': len(df_contents),
        'comments': len(df_comments),
        'top_locations': location_counter.most_common(5),
        'top_features': feature_counter.most_common(5),
        'avg_likes': df_contents['liked_count'].mean(),
        'avg_collects': df_contents['collected_count'].mean()
    }

if __name__ == "__main__":
    results = analyze_xiaohongshu_data(
        'd:/MediaCrawler-main/data/xhs/csv/search_contents_2026-01-19.csv',
        'd:/MediaCrawler-main/data/xhs/csv/search_comments_2026-01-19.csv'
    )
