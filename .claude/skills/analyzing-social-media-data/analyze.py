"""
MediaCrawler æ™ºèƒ½æ•°æ®åˆ†æå™¨
è‡ªåŠ¨è¯†åˆ«å¹³å°ç±»å‹å¹¶åŠ è½½ç›¸åº”çš„åˆ†æé…ç½®
æ”¯æŒæ¨¡æ¿åŒ–åˆ†æå’Œè‡ªå®šä¹‰å…³é”®è¯é…ç½®
"""
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import re
from collections import Counter
from typing import Dict, List, Any, Optional
import warnings
import sys
import io
warnings.filterwarnings('ignore')

# å¯¼å…¥æ¨¡æ¿åº“
try:
    from templates import (
        ANALYSIS_TEMPLATES, 
        match_template, 
        get_template, 
        get_template_keywords,
        suggest_analysis_dimensions
    )
except ImportError:
    # å¦‚æœç›´æ¥è¿è¡Œè„šæœ¬ï¼Œä½¿ç”¨ç›¸å¯¹å¯¼å…¥
    import sys
    sys.path.insert(0, str(Path(__file__).parent))
    from templates import (
        ANALYSIS_TEMPLATES, 
        match_template, 
        get_template, 
        get_template_keywords,
        suggest_analysis_dimensions
    )

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# å¹³å°åˆ†æé…ç½®åº“
# ============================================================================

PLATFORM_ANALYSIS_CONFIG = {
    'xiaohongshu': {
        'name': 'å°çº¢ä¹¦',
        'platform_keywords': ['note_id', 'xsec_token', 'collected_count'],
        'content_fields': ['title', 'desc', 'tag_list'],
        'metrics': ['liked_count', 'collected_count', 'comment_count', 'share_count'],
        'text_fields': ['title', 'desc'],
        'location_field': 'ip_location',
    },
    'douyin': {
        'name': 'æŠ–éŸ³',
        'platform_keywords': ['aweme_id', 'sec_uid'],
        'content_fields': ['title', 'desc'],
        'metrics': ['liked_count', 'comment_count', 'share_count'],
        'text_fields': ['title', 'desc'],
        'location_field': 'ip_location',
    },
    'bilibili': {
        'name': 'Bç«™',
        'platform_keywords': ['bvid', 'video_play_count'],
        'content_fields': ['title', 'desc'],
        'metrics': ['liked_count', 'video_play_count', 'video_coin_count', 'video_collect_count'],
        'text_fields': ['title', 'desc'],
        'location_field': None,
    },
    'weibo': {
        'name': 'å¾®åš',
        'platform_keywords': ['mid', 'mblogid'],
        'content_fields': ['text', 'topic_list'],
        'metrics': ['liked_count', 'comments_count', 'reposts_count'],
        'text_fields': ['text'],
        'location_field': None,
    },
    'kuaishou': {
        'name': 'å¿«æ‰‹',
        'platform_keywords': ['photo_id'],
        'content_fields': ['caption'],
        'metrics': ['liked_count', 'view_count', 'comment_count'],
        'text_fields': ['caption'],
        'location_field': None,
    },
    'tieba': {
        'name': 'è´´å§',
        'platform_keywords': ['tieba_id', 'thread_id'],
        'content_fields': ['title', 'content'],
        'metrics': ['reply_count'],
        'text_fields': ['title', 'content'],
        'location_field': None,
    },
    'zhihu': {
        'name': 'çŸ¥ä¹',
        'platform_keywords': ['answer_id', 'question_id'],
        'content_fields': ['title', 'content'],
        'metrics': ['voteup_count', 'comment_count'],
        'text_fields': ['title', 'content'],
        'location_field': None,
    }
}

# ============================================================================
# å¹³å°æ£€æµ‹å™¨
# ============================================================================

def detect_platform(contents_df: pd.DataFrame, comments_df: pd.DataFrame = None) -> str:
    """
    æ™ºèƒ½æ£€æµ‹å¹³å°ç±»å‹

    Args:
        contents_df: å†…å®¹DataFrame
        comments_df: è¯„è®ºDataFrameï¼ˆå¯é€‰ï¼‰

    Returns:
        str: å¹³å°æ ‡è¯†ç¬¦
    """
    columns = set(contents_df.columns.tolist())

    # é€šè¿‡ç‰¹å¾åˆ—åè¯†åˆ«å¹³å°
    for platform_id, config in PLATFORM_ANALYSIS_CONFIG.items():
        if any(keyword in columns for keyword in config['platform_keywords']):
            return platform_id

    return 'generic'


def extract_locations(
    text: str, 
    custom_patterns: List[str] = None
) -> List[str]:
    """
    æå–åœ°ç†ä½ç½®ä¿¡æ¯

    Args:
        text: æ–‡æœ¬å†…å®¹
        custom_patterns: è‡ªå®šä¹‰åœ°ç‚¹åŒ¹é…æ¨¡å¼åˆ—è¡¨

    Returns:
        list: æå–åˆ°çš„åœ°ç‚¹åˆ—è¡¨
    """
    locations = []

    if pd.isna(text):
        return locations

    text_str = str(text)

    # é»˜è®¤é€šç”¨åœ°ç‚¹æ¨¡å¼
    default_patterns = [
        r'(\w{2,}è·¯)',      # XXè·¯
        r'(\w{2,}å¹¿åœº)',    # XXå¹¿åœº
        r'(\w{2,}å•†åœº)',    # XXå•†åœº
        r'(\w{2,}è´­ç‰©ä¸­å¿ƒ)', # XXè´­ç‰©ä¸­å¿ƒ
        r'(\w{2,}å¤§å­¦)',    # XXå¤§å­¦
        r'(\w{2,}å…¬å›­)',    # XXå…¬å›­
        r'(\w{2,}å›¾ä¹¦é¦†)',  # XXå›¾ä¹¦é¦†
        r'åœ°é“(\w+)ç«™',     # åœ°é“XXç«™
        r'(\w{2,}åŒº)',      # XXåŒºï¼ˆåŸå¸‚è¡Œæ”¿åŒºï¼‰
    ]

    # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å¼æˆ–é»˜è®¤æ¨¡å¼
    patterns = custom_patterns if custom_patterns else default_patterns

    for pattern in patterns:
        matches = re.findall(pattern, text_str)
        locations.extend(matches)

    return locations


def analyze_features(
    text: str, 
    feature_keywords: Dict[str, List[str]] = None
) -> List[str]:
    """
    åˆ†ææ–‡æœ¬ç‰¹å¾

    Args:
        text: æ–‡æœ¬å†…å®¹
        feature_keywords: ç‰¹å¾å…³é”®è¯å­—å…¸ {ç‰¹å¾å: [å…³é”®è¯åˆ—è¡¨]}

    Returns:
        list: æå–åˆ°çš„ç‰¹å¾åˆ—è¡¨
    """
    features = []

    if pd.isna(text):
        return features

    if not feature_keywords:
        return features

    text_lower = str(text).lower()

    for feature, keywords in feature_keywords.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                features.append(feature)
                break

    return features


def analyze_sentiment(
    text: str, 
    sentiment_keywords: Dict[str, List[str]] = None
) -> str:
    """
    åˆ†ææƒ…æ„Ÿå€¾å‘

    Args:
        text: æ–‡æœ¬å†…å®¹
        sentiment_keywords: æƒ…æ„Ÿå…³é”®è¯å­—å…¸ {'positive': [...], 'negative': [...]}

    Returns:
        str: 'positive', 'negative', æˆ– 'neutral'
    """
    if pd.isna(text):
        return 'neutral'

    if not sentiment_keywords:
        return 'neutral'

    text_lower = str(text).lower()

    positive_words = sentiment_keywords.get('positive', [])
    negative_words = sentiment_keywords.get('negative', [])

    for word in positive_words:
        if word.lower() in text_lower:
            return 'positive'

    for word in negative_words:
        if word.lower() in text_lower:
            return 'negative'

    return 'neutral'


def get_project_root() -> Path:
    """é€šè¿‡æŸ¥æ‰¾ .claude æ–‡ä»¶å¤¹è‡ªåŠ¨å®šä½é¡¹ç›®æ ¹ç›®å½•"""
    current_path = Path(__file__).resolve()
    # å‘ä¸Šå¯»æ‰¾åŒ…å« .claude æ–‡ä»¶å¤¹çš„ç›®å½•
    for parent in [current_path] + list(current_path.parents):
        if (parent / ".claude").is_dir():
            return parent
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼ˆæ¯”å¦‚è„šæœ¬è¢«ç§»åˆ°äº†é¡¹ç›®å¤–ï¼‰ï¼Œåˆ™ fallback åˆ°è„šæœ¬çš„ä¸Šä¸Šä¸Šçº§æˆ–å½“å‰ç›®å½•
    return current_path.parents[3] if len(current_path.parents) > 3 else current_path.parent


def read_data_file(file_path: str) -> pd.DataFrame:
    """
    æ™ºèƒ½è¯»å–æ•°æ®æ–‡ä»¶ï¼Œè‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼ˆCSV æˆ– Excelï¼‰
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .csv, .xlsx, .xlsï¼‰
    
    Returns:
        pd.DataFrame: è¯»å–çš„æ•°æ®
    
    Raises:
        ValueError: å¦‚æœæ–‡ä»¶æ ¼å¼ä¸æ”¯æŒ
    """
    file_path = Path(file_path)
    
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    file_ext = file_path.suffix.lower()
    
    if file_ext == '.csv':
        return pd.read_csv(file_path)
    elif file_ext in ['.xlsx', '.xls']:
        try:
            return pd.read_excel(file_path)
        except ImportError:
            raise ImportError(
                "è¯»å– Excel æ–‡ä»¶éœ€è¦å®‰è£… openpyxl åº“ã€‚\n"
                "è¯·è¿è¡Œ: pip install openpyxl æˆ– uv add openpyxl"
            )
    else:
        raise ValueError(
            f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_ext}\n"
            f"æ”¯æŒçš„æ ¼å¼: .csv, .xlsx, .xls"
        )

# ============================================================================
# ä¸»åˆ†æå‡½æ•°
# ============================================================================

def analyze_mediacrawler_data(
    contents_file: str,
    comments_file: str = None,
    custom_keywords: Dict[str, Any] = None,
    template_id: str = None,
    custom_title: str = None,
    custom_location_patterns: List[str] = None,
    output_dir: str = None
) -> Dict[str, Any]:
    """
    ç»¼åˆåˆ†æMediaCrawlerçˆ¬å–çš„æ•°æ®

    Args:
        contents_file: å†…å®¹æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .csv, .xlsx, .xlsï¼‰
        comments_file: è¯„è®ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œæ”¯æŒ .csv, .xlsx, .xlsï¼‰
        custom_keywords: è‡ªå®šä¹‰å…³é”®è¯å­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                'features': {
                    'ç‰¹å¾å': ['å…³é”®è¯1', 'å…³é”®è¯2'],
                    ...
                },
                'sentiment': {
                    'positive': ['å¥½è¯1', 'å¥½è¯2'],
                    'negative': ['åè¯1', 'åè¯2']
                }
            }
        template_id: ä½¿ç”¨çš„åˆ†ææ¨¡æ¿IDï¼ˆå¦‚ 'restaurant', 'travel' ç­‰ï¼‰
        custom_title: è‡ªå®šä¹‰åˆ†ææŠ¥å‘Šæ ‡é¢˜
        custom_location_patterns: è‡ªå®šä¹‰åœ°ç‚¹åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºé¡¹ç›®æ ¹ç›®å½•ï¼‰

    Returns:
        dict: åˆ†æç»“æœå’Œå¯è§†åŒ–æ–‡ä»¶è·¯å¾„
    """
    # è¯»å–æ•°æ®ï¼ˆè‡ªåŠ¨æ£€æµ‹æ ¼å¼ï¼‰
    df_contents = read_data_file(contents_file)
    df_comments = read_data_file(comments_file) if comments_file else None

    # æ™ºèƒ½æ£€æµ‹å¹³å°
    platform = detect_platform(df_contents, df_comments)
    platform_config = PLATFORM_ANALYSIS_CONFIG.get(platform, {})
    platform_name = platform_config.get('name', platform.title())

    # ç¡®å®šåˆ†æå…³é”®è¯é…ç½®
    if custom_keywords:
        # ä½¿ç”¨ç”¨æˆ·è‡ªå®šä¹‰å…³é”®è¯
        analysis_keywords = custom_keywords
    elif template_id:
        # ä½¿ç”¨æŒ‡å®šæ¨¡æ¿
        analysis_keywords = get_template_keywords(template_id)
    else:
        # å°è¯•ä»æ•°æ®ä¸­æ¨æ–­æ¨¡æ¿
        # æ£€æŸ¥æ˜¯å¦æœ‰ source_keyword å­—æ®µ
        if 'source_keyword' in df_contents.columns:
            sample_keywords = df_contents['source_keyword'].dropna().head(5).tolist()
            keywords_str = ' '.join(sample_keywords)
            template_id = match_template(keywords_str)
        else:
            template_id = 'generic'
        analysis_keywords = get_template_keywords(template_id)

    feature_keywords = analysis_keywords.get('features', {})
    sentiment_keywords = analysis_keywords.get('sentiment', {})

    # è·å–åœ°ç‚¹åŒ¹é…æ¨¡å¼
    if custom_location_patterns:
        location_patterns = custom_location_patterns
    elif template_id:
        template = get_template(template_id)
        location_patterns = template.get('location_patterns', [])
    else:
        location_patterns = None

    # ç¡®å®šè¾“å‡ºç›®å½•
    project_root = get_project_root()
    if not output_dir:
        output_dir = project_root / "REPORT"
    else:
        output_dir = Path(output_dir)
    
    output_dir.mkdir(parents=True, exist_ok=True)

    # è‡ªå®šä¹‰æ ‡é¢˜
    if not custom_title:
        template_name = get_template(template_id or 'generic').get('name', '')
        custom_title = f"ğŸ“Š {platform_name}æ•°æ®åˆ†ææŠ¥å‘Š - {template_name}"

    # å¼€å§‹æ•è·è¾“å‡ºä»¥ä¿å­˜åˆ°æ–‡ä»¶
    report_output = io.StringIO()
    
    def smart_print(*args, **kwargs):
        print(*args, **kwargs)
        print(*args, file=report_output, **kwargs)

    smart_print("=" * 80)
    smart_print(custom_title)
    smart_print("=" * 80)

    # æ•°æ®æ¦‚è§ˆ
    smart_print(f"\nâœ… å¹³å°è¯†åˆ«: {platform_name} ({platform})")
    if template_id:
        smart_print(f"âœ… åˆ†ææ¨¡æ¿: {get_template(template_id).get('name', template_id)}")
    smart_print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
    smart_print(f"   å¸–å­æ•°æ®: {len(df_contents)} æ¡")
    if df_comments is not None:
        smart_print(f"   è¯„è®ºæ•°æ®: {len(df_comments)} æ¡")

    # 1. åŸºç¡€ç»Ÿè®¡
    smart_print("\n" + "=" * 80)
    smart_print("ğŸ“ˆ ä¸€ã€åŸºç¡€æ•°æ®ç»Ÿè®¡")
    smart_print("=" * 80)

    metrics = platform_config.get('metrics', [])
    metric_names = [m for m in metrics if m in df_contents.columns]

    if metric_names:
        smart_print(f"\näº’åŠ¨æ•°æ®ç»Ÿè®¡:")
        for metric in metric_names:
            mean_val = df_contents[metric].mean()
            max_val = df_contents[metric].max()
            metric_label = metric.replace('_', ' ').title()
            smart_print(f"  å¹³å‡{metric_label}: {mean_val:.1f}")
            smart_print(f"  æœ€é«˜{metric_label}: {max_val}")

    # 2. åœ°ç†ä½ç½®åˆ†æ
    smart_print("\n" + "=" * 80)
    smart_print("ğŸ“ äºŒã€åœ°ç†ä½ç½®åˆ†æ")
    smart_print("=" * 80)

    location_field = platform_config.get('location_field')
    all_locations = []
    location_counter = Counter()

    if location_field and location_field in df_contents.columns:
        # ä»ä¸“ç”¨å­—æ®µæå–
        locations_data = df_contents[location_field].value_counts().head(10)
        smart_print(f"\n{location_field} åˆ†å¸ƒ Top 10:")
        for loc, count in locations_data.items():
            if pd.notna(loc):
                smart_print(f"  {loc}: {count} æ¬¡")
                location_counter[loc] = count
    
    # ä»æ–‡æœ¬ä¸­æå–åœ°ç‚¹
    text_fields = platform_config.get('text_fields', [])
    for idx, row in df_contents.iterrows():
        text = ' '.join([str(row.get(field, '')) for field in text_fields])
        locations = extract_locations(text, location_patterns)
        all_locations.extend(locations)

    if all_locations:
        location_counter = Counter(all_locations)
        smart_print(f"\næ–‡æœ¬ä¸­æåŠçš„åœ°ç‚¹ Top 10:")
        for location, count in location_counter.most_common(10):
            smart_print(f"  {location}: {count} æ¬¡")

    # 3. ç‰¹å¾åˆ†æ
    smart_print("\n" + "=" * 80)
    smart_print("ğŸ¯ ä¸‰ã€å†…å®¹ç‰¹å¾åˆ†æ")
    smart_print("=" * 80)

    all_features = []

    for idx, row in df_contents.iterrows():
        text = ' '.join([str(row.get(field, '')) for field in text_fields])
        features = analyze_features(text, feature_keywords)
        all_features.extend(features)

    feature_counter = Counter(all_features)

    if feature_counter:
        smart_print(f"\nç‰¹å¾æåŠæ¬¡æ•° Top 10:")
        for feature, count in feature_counter.most_common(10):
            smart_print(f"  {feature}: {count} æ¬¡")
    else:
        smart_print("\næœªæ£€æµ‹åˆ°æ˜¾è‘—ç‰¹å¾ï¼ˆå¯ä½¿ç”¨custom_keywordså‚æ•°æˆ–template_idæŒ‡å®šåˆ†ææ¨¡æ¿ï¼‰")

    # 4. æƒ…æ„Ÿåˆ†æ
    sentiment_results = {'positive': 0, 'negative': 0, 'neutral': 0}
    positive_pct = 0.0

    if df_comments is not None:
        smart_print("\n" + "=" * 80)
        smart_print("ğŸ’¬ å››ã€è¯„è®ºæƒ…æ„Ÿåˆ†æ")
        smart_print("=" * 80)

        for idx, row in df_comments.head(200).iterrows():
            comment = row.get('content', '')
            sentiment = analyze_sentiment(comment, sentiment_keywords)
            sentiment_results[sentiment] += 1

        total = sentiment_results['positive'] + sentiment_results['negative']
        positive_pct = (sentiment_results['positive'] / total * 100) if total > 0 else 0

        smart_print(f"\nè¯„è®ºæƒ…æ„Ÿåˆ†å¸ƒ (åŸºäºå‰200æ¡è¯„è®º):")
        smart_print(f"  ç§¯æ: {sentiment_results['positive']} æ¡")
        smart_print(f"  æ¶ˆæ: {sentiment_results['negative']} æ¡")
        smart_print(f"  ä¸­æ€§: {sentiment_results['neutral']} æ¡")
        if total > 0:
            smart_print(f"  ç§¯æå æ¯”: {positive_pct:.1f}%")

    # 5. åˆ›å»ºå¯è§†åŒ–
    smart_print("\n" + "=" * 80)
    smart_print("ğŸ“Š äº”ã€ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    smart_print("=" * 80)

    output_file = create_visualizations(
        df_contents,
        df_comments,
        platform,
        platform_config,
        location_counter if (all_locations or location_counter) else None,
        feature_counter,
        output_dir
    )

    smart_print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜: {output_file}")

    # 6. çƒ­é—¨å†…å®¹
    smart_print("\n" + "=" * 80)
    smart_print("ğŸ”¥ å…­ã€çƒ­é—¨å†…å®¹ Top 3")
    smart_print("=" * 80)

    if metric_names:
        primary_metric = metric_names[0]
        top_contents = df_contents.nlargest(3, primary_metric)

        for idx, row in top_contents.iterrows():
            title = row.get('title', row.get('text', row.get('caption', 'N/A')))
            smart_print(f"\n  {str(title)[:60]}...")
            smart_print(f"  ğŸ‘ {row[primary_metric]} {primary_metric}")

    # 7. å…³é”®æ´å¯Ÿ
    smart_print("\n" + "=" * 80)
    smart_print("ğŸ’¡ ä¸ƒã€å…³é”®æ´å¯Ÿ")
    smart_print("=" * 80)

    insights = []

    if feature_counter:
        top_feature = feature_counter.most_common(1)[0]
        insights.append(f"ç”¨æˆ·æœ€å…³æ³¨: {top_feature[0]} (æåŠ{top_feature[1]}æ¬¡)")

    if location_counter:
        top_location = location_counter.most_common(1)[0]
        insights.append(f"æœ€çƒ­é—¨åŒºåŸŸ: {top_location[0]} (æåŠ{top_location[1]}æ¬¡)")

    if df_comments is not None and total > 0:
        insights.append(f"è¯„è®ºæƒ…æ„Ÿå€¾å‘: ç§¯æ{positive_pct:.1f}%")

    if metric_names and len(metric_names) >= 2:
        metric1_mean = df_contents[metric_names[0]].mean()
        metric2_mean = df_contents[metric_names[1]].mean()
        ratio = metric2_mean / metric1_mean if metric1_mean > 0 else 0
        insights.append(f"{metric_names[1]}æ˜¯{metric_names[0]}çš„{ratio:.2f}å€")

    for insight in insights:
        smart_print(f"  â€¢ {insight}")

    smart_print("\n" + "=" * 80)
    smart_print("âœ… åˆ†æå®Œæˆ!")
    smart_print("=" * 80)

    # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
    report_file = output_dir / f"{platform}_report.txt"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_output.getvalue())
    print(f"âœ… æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

    return {
        'platform': platform,
        'template_id': template_id,
        'contents_count': len(df_contents),
        'comments_count': len(df_comments) if df_comments is not None else 0,
        'top_features': feature_counter.most_common(5),
        'top_locations': location_counter.most_common(5) if location_counter else [],
        'sentiment': sentiment_results,
        'visualization': output_file,
        'insights': insights
    }


def create_visualizations(
    df_contents: pd.DataFrame,
    df_comments: pd.DataFrame,
    platform: str,
    platform_config: Dict[str, Any],
    location_counter: Counter,
    feature_counter: Counter,
    output_dir: str = None
) -> str:
    """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""

    fig = plt.figure(figsize=(16, 12))
    metrics = platform_config.get('metrics', [])
    available_metrics = [m for m in metrics if m in df_contents.columns]

    # å›¾1: äº’åŠ¨æ•°æ®åˆ†å¸ƒ
    ax1 = plt.subplot(2, 3, 1)
    if available_metrics:
        for metric in available_metrics[:3]:
            data = df_contents[metric].dropna()
            if len(data) > 0:
                plt.hist(data, bins=20, alpha=0.5, label=metric.replace('_', ' ').title())
        plt.xlabel('æ•°é‡')
        plt.ylabel('å¸–å­æ•°')
        plt.title('äº’åŠ¨æ•°æ®åˆ†å¸ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)

    # å›¾2: äº’åŠ¨æ•°æ®ç›¸å…³æ€§
    ax2 = plt.subplot(2, 3, 2)
    if len(available_metrics) > 1:
        corr_data = df_contents[available_metrics].dropna()
        if len(corr_data) > 0:
            corr = corr_data.corr()
            sns.heatmap(corr, annot=True, cmap='coolwarm', center=0,
                       square=True, linewidths=1, ax=ax2, cbar_kws={'shrink': 0.8})
            ax2.set_title('äº’åŠ¨æ•°æ®ç›¸å…³æ€§')

    # å›¾3: çƒ­é—¨åŒºåŸŸ/åœ°ç‚¹
    ax3 = plt.subplot(2, 3, 3)
    if location_counter:
        top_locations = dict(location_counter.most_common(10))
        if top_locations:
            plt.barh(range(len(top_locations)), list(top_locations.values()))
            plt.yticks(range(len(top_locations)), list(top_locations.keys()))
            plt.xlabel('æåŠæ¬¡æ•°')
            plt.title('çƒ­é—¨åœ°ç‚¹ Top 10')
            plt.grid(True, alpha=0.3, axis='x')

    # å›¾4: å†…å®¹ç‰¹å¾æ’å
    ax4 = plt.subplot(2, 3, 4)
    if feature_counter:
        top_features = dict(feature_counter.most_common(10))
        if top_features:
            plt.barh(range(len(top_features)), list(top_features.values()))
            plt.yticks(range(len(top_features)), list(top_features.keys()))
            plt.xlabel('æåŠæ¬¡æ•°')
            plt.title('å†…å®¹ç‰¹å¾ Top 10')
            plt.grid(True, alpha=0.3, axis='x')

    # å›¾5: IPåœ°ç‚¹åˆ†å¸ƒï¼ˆå¦‚æœæœ‰ï¼‰
    ax5 = plt.subplot(2, 3, 5)
    location_field = platform_config.get('location_field')
    if location_field and location_field in df_contents.columns:
        ip_locations = df_contents[location_field].value_counts().head(10)
        if len(ip_locations) > 0:
            plt.barh(range(len(ip_locations)), ip_locations.values)
            plt.yticks(range(len(ip_locations)), ip_locations.index)
            plt.xlabel('å¸–å­æ•°')
            plt.title(f'{location_field} åˆ†å¸ƒ')
            plt.grid(True, alpha=0.3, axis='x')

    # å›¾6: æ•°æ®è´¨é‡
    ax6 = plt.subplot(2, 3, 6)
    missing_data = df_contents.isnull().sum()
    missing_data = missing_data[missing_data > 0].sort_values(ascending=True)
    if len(missing_data) > 0:
        plt.barh(range(len(missing_data)), missing_data.values)
        plt.yticks(range(len(missing_data)), missing_data.index)
        plt.xlabel('ç¼ºå¤±æ•°é‡')
        plt.title('æ•°æ®ç¼ºå¤±æƒ…å†µ')
        plt.grid(True, alpha=0.3, axis='x')
    else:
        ax6.text(0.5, 0.5, 'æ•°æ®å®Œæ•´\næ— ç¼ºå¤±', ha='center', va='center',
                fontsize=14, transform=ax6.transAxes)
        ax6.set_title('æ•°æ®è´¨é‡')

    plt.tight_layout()

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    output_path = Path(output_dir) if output_dir else get_project_root()
    output_file = str(output_path / f'{platform}_analysis.png')
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    plt.close()

    return output_file


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼ˆä¾› AI æ¨ç†ä½¿ç”¨ï¼‰
# ============================================================================

def preview_data_structure(contents_file: str) -> Dict[str, Any]:
    """
    é¢„è§ˆæ•°æ®ç»“æ„ï¼Œç”¨äº AI æ¨ç†åˆ†ææ–¹å‘
    
    Args:
        contents_file: å†…å®¹æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .csv, .xlsx, .xlsï¼‰
    
    Returns:
        dict: æ•°æ®ç»“æ„é¢„è§ˆä¿¡æ¯
    """
    df = read_data_file(contents_file)
    platform = detect_platform(df)
    
    # è·å–æœç´¢å…³é”®è¯
    keywords = []
    if 'source_keyword' in df.columns:
        keywords = df['source_keyword'].dropna().unique().tolist()
    
    # æ¨èåˆ†ææ¨¡æ¿
    keywords_str = ' '.join(keywords) if keywords else ''
    suggested = suggest_analysis_dimensions(keywords_str)
    
    return {
        'platform': platform,
        'platform_name': PLATFORM_ANALYSIS_CONFIG.get(platform, {}).get('name', platform),
        'row_count': len(df),
        'columns': list(df.columns),
        'search_keywords': keywords,
        'suggested_template': suggested['recommended_template'],
        'suggested_template_name': suggested['template_name'],
        'suggested_features': suggested['suggested_features'],
        'sample_titles': df['title'].head(5).tolist() if 'title' in df.columns else []
    }


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

if __name__ == "__main__":
    import sys

    # ç®€å•çš„å‘½ä»¤è¡Œæ¥å£
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python analyze.py <contentsæ–‡ä»¶> [commentsæ–‡ä»¶] [--template=<template_id>]")
        print("æ”¯æŒæ ¼å¼: .csv, .xlsx, .xls")
        print("\nå¯ç”¨æ¨¡æ¿:")
        from templates import list_templates
        for t in list_templates():
            print(f"  {t['id']}: {t['name']}")
        sys.exit(1)

    contents_file = sys.argv[1]
    comments_file = None
    template_id = None

    # è§£æå‚æ•°
    for arg in sys.argv[2:]:
        if arg.startswith('--template='):
            template_id = arg.split('=')[1]
        elif not arg.startswith('--'):
            comments_file = arg

    results = analyze_mediacrawler_data(
        contents_file, 
        comments_file,
        template_id=template_id
    )
