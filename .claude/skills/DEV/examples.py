#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æå–å·¥å…· - å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

è¿™ä¸ªæ–‡ä»¶åŒ…å«äº†å¸¸è§çš„ä½¿ç”¨åœºæ™¯ï¼Œå¯ä»¥ç›´æ¥å¤åˆ¶ä¿®æ”¹
"""

from extractor import UniversalExtractor


# ============================================================================
# ç¤ºä¾‹1ï¼šæå–å’–å•¡å…å“ç‰Œ
# ============================================================================

def example_1_cafe_brands():
    """æå–å’–å•¡å…å“ç‰Œ"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹1ï¼šæå–å’–å•¡å…å“ç‰Œ")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # æå–å“ç‰Œ
    brands = ['ä¸€å°ºèŠ±å›­', 'æ˜Ÿå·´å…‹', 'Manner', 'ç‘å¹¸', 'Costa']
    results = extractor.extract_by_keywords(
        keywords=brands,
        top_n=30
    )

    extractor.print_results(results, title="å’–å•¡å…å“ç‰Œæå–")

    # ä¿å­˜ç»“æœ
    extractor.save_results(results, 'output/brands.json')


# ============================================================================
# ç¤ºä¾‹2ï¼šæå–ä»·æ ¼ä¿¡æ¯
# ============================================================================

def example_2_extract_prices():
    """æå–ä»·æ ¼ä¿¡æ¯"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹2ï¼šæå–ä»·æ ¼ä¿¡æ¯")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # æå–ä»·æ ¼
    prices = extractor.extract_prices(top_n=50)

    extractor.print_results(prices, title="ä»·æ ¼ä¿¡æ¯æå–")

    # ä¿å­˜ä¸ºCSVï¼ˆæ–¹ä¾¿Excelæ‰“å¼€ï¼‰
    extractor.save_results(prices, 'output/prices.csv', format='csv')


# ============================================================================
# ç¤ºä¾‹3ï¼šåˆ†æåœ°ç†ä½ç½®åˆ†å¸ƒ
# ============================================================================

def example_3_location_analysis():
    """åˆ†æåœ°ç†ä½ç½®åˆ†å¸ƒ"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹3ï¼šåœ°ç†ä½ç½®åˆ†æ")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # è‡ªå®šä¹‰åœ°ç‚¹å…³é”®è¯
    locations = extractor.extract_locations(
        location_keywords=[
            'å®å±±åŒº', 'æ·æ²ªé“è·¯', 'æ™ºæ…§æ¹¾', 'é¡¾æ‘', 'æ¨è¡Œ',
            'å´æ·', 'å¤§åœº', 'ä¸Šæµ·å¤§å­¦', 'å®æ¨è·¯', 'å‹è°Šè·¯'
        ],
        top_n=20
    )

    print("\nğŸ“ åœ°ç†ä½ç½®åˆ†å¸ƒ:")
    print("-"*80)
    for location, count in locations.items():
        print(f"{location}: {count}æ¬¡")


# ============================================================================
# ç¤ºä¾‹4ï¼šæŸ¥æ‰¾é«˜æ€§ä»·æ¯”åº—é“º
# ============================================================================

def example_4_value_for_money():
    """æŸ¥æ‰¾é«˜æ€§ä»·æ¯”åº—é“º"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹4ï¼šæŸ¥æ‰¾é«˜æ€§ä»·æ¯”åº—é“º")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # å…ˆæå–ä»·æ ¼ä¿¡æ¯
    prices = extractor.extract_prices(top_n=100)

    # è¿‡æ»¤å‡ºä½ä»·ä½ï¼ˆ<50å…ƒæˆ–å…è´¹ï¼‰
    value_shops = []
    for item in prices:
        price_text = item.get('price', '')

        if 'å…è´¹' in price_text:
            value_shops.append(item)
        elif any(num in price_text for num in ['11', '20', '30']):
            value_shops.append(item)

    extractor.print_results(value_shops[:20], title="é«˜æ€§ä»·æ¯”åº—é“º")

    extractor.save_results(value_shops, 'output/value_shops.json')


# ============================================================================
# ç¤ºä¾‹5ï¼šæå–çƒ­é—¨å¸–å­
# ============================================================================

def example_5_top_posts():
    """æå–çƒ­é—¨å¸–å­"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹5ï¼šæå–çƒ­é—¨å¸–å­")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # æå–ç‚¹èµ>1000çš„å¸–å­
    top_posts = extractor.extract_top_posts(
        top_n=20,
        min_likes=1000
    )

    extractor.print_results(top_posts, title="çƒ­é—¨å¸–å­ TOP 20")

    extractor.save_results(top_posts, 'output/top_posts.json')


# ============================================================================
# ç¤ºä¾‹6ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–ç‰¹å®šä¿¡æ¯
# ============================================================================

def example_6_regex_pattern():
    """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹6ï¼šæ­£åˆ™è¡¨è¾¾å¼æå–")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # æå–è¥ä¸šæ—¶é—´ä¿¡æ¯
    results = extractor.extract_by_pattern(
        pattern=r'(\d+:\d+\s*[-è‡³]\s*\d+:\d+)',  # åŒ¹é… 10:00-20:00
        search_fields=['desc'],
        top_n=20
    )

    extractor.print_results(results, title="è¥ä¸šæ—¶é—´ä¿¡æ¯")


# ============================================================================
# ç¤ºä¾‹7ï¼šè‡ªå®šä¹‰æå–é€»è¾‘
# ============================================================================

def example_7_custom_extraction():
    """è‡ªå®šä¹‰æå–é€»è¾‘"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹7ï¼šè‡ªå®šä¹‰æå–")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # è‡ªå®šä¹‰è¿‡æ»¤å’Œæå–å‡½æ•°
    def filter_function(row):
        """åªä¿ç•™åŒ…å«'å®å±±'ä¸”ç‚¹èµ>100çš„å¸–å­"""
        text = str(row.get('title', '')) + str(row.get('desc', ''))
        return 'å®å±±' in text and row.get('liked_count', 0) > 100

    def extract_function(row):
        """æå–è‡ªå®šä¹‰å­—æ®µ"""
        title = row.get('title', '')
        desc = str(row.get('desc', ''))

        # æå–åœ°å€
        import re
        address_match = re.search(r'åœ°å€[ï¼š:]\s*(.*?)(?:\n|$)', desc)
        address = address_match.group(1) if address_match else 'æœªæ‰¾åˆ°'

        return {
            'title': title,
            'liked': row.get('liked_count', 0),
            'address': address[:50],
            'has_plug': 'æ’åº§' in desc or 'ç”µæº' in desc,
            'has_wifi': 'WiFi' in desc or 'wifi' in desc or 'æ— çº¿' in desc,
        }

    results = extractor.extract_custom(
        filter_func=filter_function,
        extract_func=extract_function,
        top_n=20
    )

    extractor.print_results(results, title="å®å±±åŒºåº—é“ºè¯¦æƒ…")

    extractor.save_results(results, 'output/custom_extraction.json')


# ============================================================================
# ç¤ºä¾‹8ï¼šç»Ÿè®¡åˆ†æ
# ============================================================================

def example_8_statistics():
    """ç»Ÿè®¡åˆ†æ"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹8ï¼šç»Ÿè®¡åˆ†æ")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = extractor.extract_statistics()

    print("\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
    print("-"*80)
    for key, value in stats.items():
        print(f"{key}: {value:,}")


# ============================================================================
# ç¤ºä¾‹9ï¼šç»„åˆå¤šä¸ªæ“ä½œ
# ============================================================================

def example_9_combined_analysis():
    """ç»„åˆåˆ†ææµç¨‹"""
    print("\n" + "="*80)
    print("ç¤ºä¾‹9ï¼šç»„åˆåˆ†æ")
    print("="*80)

    extractor = UniversalExtractor(
        contents_file=r'd:\MediaCrawler-main\data\xhs\csv\search_contents_2026-01-20.csv'
    )

    # æ­¥éª¤1ï¼šæå–å®å±±ç›¸å…³å¸–å­
    baoshan_posts = extractor.extract_by_keywords(
        keywords=['å®å±±'],
        top_n=50
    )

    # æ­¥éª¤2ï¼šæå–ä»·æ ¼ä¿¡æ¯
    all_prices = extractor.extract_prices(top_n=100)

    # æ­¥éª¤3ï¼šæå–åœ°ç†ä½ç½®
    locations = extractor.extract_locations(top_n=15)

    # æ­¥éª¤4ï¼šæå–çƒ­é—¨å¸–å­
    top_posts = extractor.extract_top_posts(top_n=10)

    # æ‰“å°ç»¼åˆæŠ¥å‘Š
    print("\n" + "="*80)
    print("ğŸ“Š å®å±±åŒºåŸŸç»¼åˆåˆ†ææŠ¥å‘Š")
    print("="*80)

    print(f"\n1ï¸âƒ£ å®å±±ç›¸å…³å¸–å­: {len(baoshan_posts)} æ¡")
    print(f"2ï¸âƒ£ ä»·æ ¼ä¿¡æ¯: {len(all_prices)} æ¡")
    print(f"3ï¸âƒ£ æ¶‰åŠåœ°ç‚¹: {len(locations)} ä¸ª")
    print(f"4ï¸âƒ£ çƒ­é—¨å¸–å­: {len(top_posts)} æ¡")

    print("\nğŸ“ TOP 5 åœ°ç‚¹:")
    for i, (loc, count) in enumerate(list(locations.items())[:5], 1):
        print(f"   {i}. {loc}: {count}æ¬¡")

    print("\nğŸ’° TOP 5 ä»·æ ¼ä¿¡æ¯:")
    for i, price in enumerate(all_prices[:5], 1):
        print(f"   {i}. {price['title'][:40]}... | {price['price']}")

    # ä¿å­˜ç»¼åˆæŠ¥å‘Š
    report = {
        'summary': {
            'baoshan_posts': len(baoshan_posts),
            'price_info': len(all_prices),
            'locations': len(locations),
            'top_posts': len(top_posts)
        },
        'details': {
            'locations': locations,
            'top_prices': all_prices[:10],
            'top_posts': top_posts[:10]
        }
    }

    extractor.save_results(report, 'output/combined_report.json')


# ============================================================================
# ä¸»ç¨‹åº
# ============================================================================

if __name__ == '__main__':
    import sys

    # å¯é€šè¿‡å‘½ä»¤è¡Œå‚æ•°é€‰æ‹©è¿è¡Œå“ªä¸ªç¤ºä¾‹
    if len(sys.argv) > 1:
        example_num = sys.argv[1]
        examples = {
            '1': example_1_cafe_brands,
            '2': example_2_extract_prices,
            '3': example_3_location_analysis,
            '4': example_4_value_for_money,
            '5': example_5_top_posts,
            '6': example_6_regex_pattern,
            '7': example_7_custom_extraction,
            '8': example_8_statistics,
            '9': example_9_combined_analysis,
        }

        if example_num in examples:
            examples[example_num]()
        else:
            print(f"âŒ æœªæ‰¾åˆ°ç¤ºä¾‹ {example_num}")
            print("å¯ç”¨ç¤ºä¾‹: 1-9")
    else:
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        print("\n" + "="*80)
        print("ğŸš€ è¿è¡Œæ‰€æœ‰ç¤ºä¾‹")
        print("="*80)

        # åˆ›å»ºè¾“å‡ºç›®å½•
        import os
        os.makedirs('output', exist_ok=True)

        # è¿è¡Œç¤ºä¾‹
        example_1_cafe_brands()
        example_2_extract_prices()
        example_3_location_analysis()
        example_4_value_for_money()
        example_5_top_posts()
        example_8_statistics()
        example_9_combined_analysis()

        print("\n" + "="*80)
        print("âœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("ğŸ“ ç»“æœå·²ä¿å­˜åˆ° output/ ç›®å½•")
        print("="*80)
