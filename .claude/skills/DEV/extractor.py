#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨æ•°æ®æå–å·¥å…· - Universal Data Extractor
æ”¯æŒä»MediaCrawlerçˆ¬å–çš„CSVæ•°æ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯

ä½œè€…: Claude Code
ç”¨é€”: ç¤¾äº¤åª’ä½“æ•°æ®åˆ†æçš„é€šç”¨æå–å·¥å…·
"""

import pandas as pd
import re
import json
from pathlib import Path
from typing import List, Dict, Any, Optional, Callable
from collections import defaultdict
import argparse


class UniversalExtractor:
    """é€šç”¨æ•°æ®æå–å™¨"""

    def __init__(self, contents_file: str, comments_file: Optional[str] = None):
        """
        åˆå§‹åŒ–æå–å™¨

        Args:
            contents_file: å¸–å­å†…å®¹CSVæ–‡ä»¶è·¯å¾„
            comments_file: è¯„è®ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        self.contents_file = contents_file
        self.comments_file = comments_file
        self.df_contents = None
        self.df_comments = None

        self._load_data()

    def _load_data(self):
        """åŠ è½½æ•°æ®"""
        print(f"ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶...")
        self.df_contents = pd.read_csv(self.contents_file)
        print(f"âœ… å¸–å­æ•°æ®: {len(self.df_contents)} æ¡")

        if self.comments_file:
            self.df_comments = pd.read_csv(self.comments_file)
            print(f"âœ… è¯„è®ºæ•°æ®: {len(self.df_comments)} æ¡")

    def extract_by_keywords(
        self,
        keywords: List[str],
        search_fields: List[str] = ['title', 'desc'],
        top_n: int = 20,
        sort_by: str = 'liked_count',
        ascending: bool = False
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®å…³é”®è¯æå–æ•°æ®

        Args:
            keywords: å…³é”®è¯åˆ—è¡¨
            search_fields: æœç´¢çš„å­—æ®µåˆ—è¡¨
            top_n: è¿”å›å‰Næ¡ç»“æœ
            sort_by: æ’åºå­—æ®µ
            ascending: æ˜¯å¦å‡åº

        Returns:
            æå–çš„ç»“æœåˆ—è¡¨
        """
        results = []

        for idx, row in self.df_contents.iterrows():
            # åœ¨æŒ‡å®šå­—æ®µä¸­æœç´¢å…³é”®è¯
            for field in search_fields:
                text = str(row.get(field, ''))
                for keyword in keywords:
                    if keyword.lower() in text.lower():
                        results.append({
                            'title': row.get('title', ''),
                            'desc': str(row.get('desc', ''))[:200],
                            'liked': row.get('liked_count', 0),
                            'collected': row.get('collected_count', 0),
                            'comment_count': row.get('comment_count', 0),
                            'note_id': row.get('note_id', ''),
                            'matched_keyword': keyword,
                            'matched_field': field
                        })
                        break  # é¿å…é‡å¤æ·»åŠ 

        # å»é‡
        seen = set()
        unique_results = []
        for item in results:
            key = (item['title'], item['note_id'])
            if key not in seen:
                seen.add(key)
                unique_results.append(item)

        # æ’åº
        unique_results.sort(key=lambda x: x.get(sort_by, 0), reverse=not ascending)

        return unique_results[:top_n]

    def extract_by_pattern(
        self,
        pattern: str,
        search_fields: List[str] = ['title', 'desc'],
        top_n: int = 20,
        sort_by: str = 'liked_count'
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ®æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼æå–æ•°æ®

        Args:
            pattern: æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
            search_fields: æœç´¢çš„å­—æ®µåˆ—è¡¨
            top_n: è¿”å›å‰Næ¡ç»“æœ
            sort_by: æ’åºå­—æ®µ

        Returns:
            æå–çš„ç»“æœåˆ—è¡¨
        """
        results = []
        regex = re.compile(pattern)

        for idx, row in self.df_contents.iterrows():
            for field in search_fields:
                text = str(row.get(field, ''))
                match = regex.search(text)

                if match:
                    # æå–åŒ¹é…çš„æ–‡æœ¬
                    matched_text = match.group(0) if match.groups() is None else match.group(1)

                    results.append({
                        'title': row.get('title', ''),
                        'desc': str(row.get('desc', ''))[:200],
                        'liked': row.get('liked_count', 0),
                        'matched_text': matched_text,
                        'note_id': row.get('note_id', '')
                    })
                    break

        results.sort(key=lambda x: x.get(sort_by, 0), reverse=True)
        return results[:top_n]

    def extract_prices(
        self,
        price_patterns: Optional[List[tuple]] = None,
        top_n: int = 20
    ) -> List[Dict[str, Any]]:
        """
        æå–ä»·æ ¼ä¿¡æ¯

        Args:
            price_patterns: è‡ªå®šä¹‰ä»·æ ¼æ¨¡å¼åˆ—è¡¨ï¼Œæ ¼å¼: [(æ­£åˆ™, ä»·æ ¼ç±»å‹), ...]
            top_n: è¿”å›å‰Næ¡ç»“æœ

        Returns:
            ä»·æ ¼ä¿¡æ¯åˆ—è¡¨
        """
        if price_patterns is None:
            # é»˜è®¤ä»·æ ¼æ¨¡å¼
            price_patterns = [
                (r'(\d+)å…ƒ.*å¤©', 'å…ƒ/å¤©'),
                (r'(\d+)å—.*å¤©', 'å—/å¤©'),
                (r'(\d+)å…ƒ.*å°æ—¶', 'å…ƒ/å°æ—¶'),
                (r'(\d+)å—é’±', 'å…ƒ'),
                (r'å…è´¹', 'å…è´¹'),
                (r'(\d+)æ¬¡.*å¡', 'æ¬¡å¡'),
                (r'(\d+).*æœˆå¡', 'æœˆå¡'),
            ]

        results = []

        for idx, row in self.df_contents.iterrows():
            text = row.get('title', '') + ' ' + str(row.get('desc', ''))

            for pattern, price_type in price_patterns:
                match = re.search(pattern, text)
                if match:
                    if 'å…è´¹' in pattern:
                        price_value = 'å…è´¹'
                    else:
                        price_value = match.group(1) + ' ' + price_type

                    results.append({
                        'title': row.get('title', ''),
                        'price': price_value,
                        'price_type': price_type,
                        'liked': row.get('liked_count', 0),
                        'desc': str(row.get('desc', ''))[:200]
                    })
                    break

        results.sort(key=lambda x: x.get('liked', 0), reverse=True)
        return results[:top_n]

    def extract_locations(
        self,
        location_keywords: Optional[List[str]] = None,
        top_n: int = 10
    ) -> Dict[str, int]:
        """
        æå–åœ°ç†ä½ç½®ä¿¡æ¯

        Args:
            location_keywords: è‡ªå®šä¹‰åœ°ç‚¹å…³é”®è¯åˆ—è¡¨
            top_n: è¿”å›å‰Nä¸ªåœ°ç‚¹

        Returns:
            åœ°ç‚¹åŠå…¶å‡ºç°æ¬¡æ•°çš„å­—å…¸
        """
        location_counts = defaultdict(int)

        for idx, row in self.df_contents.iterrows():
            text = row.get('title', '') + ' ' + str(row.get('desc', ''))

            if location_keywords:
                keywords = location_keywords
            else:
                # é»˜è®¤æå–å¸¸è§åœ°ç‚¹æ¨¡å¼
                keywords = re.findall(r'(ä¸Šæµ·.*?åŒº|ä¸Šæµ·.*?è·¯|ä¸Šæµ·.*?å¹¿åœº|ä¸Šæµ·.*?å…¬å›­|.*?åŒº|.*?è·¯|.*?å¹¿åœº|.*?å…¬å›­)', text)

            for keyword in keywords:
                if keyword in text:
                    location_counts[keyword] += 1

        # æ’åº
        sorted_locations = sorted(location_counts.items(), key=lambda x: x[1], reverse=True)
        return dict(sorted_locations[:top_n])

    def extract_top_posts(
        self,
        top_n: int = 20,
        min_likes: int = 0
    ) -> List[Dict[str, Any]]:
        """
        æå–çƒ­é—¨å¸–å­

        Args:
            top_n: è¿”å›å‰Næ¡
            min_likes: æœ€å°ç‚¹èµæ•°è¿‡æ»¤

        Returns:
            çƒ­é—¨å¸–å­åˆ—è¡¨
        """
        df_filtered = self.df_contents[self.df_contents.get('liked_count', 0) >= min_likes]
        df_sorted = df_filtered.sort_values('liked_count', ascending=False)

        results = []
        for idx, row in df_sorted.head(top_n).iterrows():
            results.append({
                'title': row.get('title', ''),
                'liked': row.get('liked_count', 0),
                'collected': row.get('collected_count', 0),
                'comments': row.get('comment_count', 0),
                'note_id': row.get('note_id', ''),
                'desc': str(row.get('desc', ''))[:300]
            })

        return results

    def extract_statistics(self) -> Dict[str, Any]:
        """
        æå–æ•°æ®ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            'total_posts': len(self.df_contents),
            'total_likes': int(self.df_contents.get('liked_count', 0).sum()),
            'avg_likes': float(self.df_contents.get('liked_count', 0).mean()),
            'max_likes': int(self.df_contents.get('liked_count', 0).max()),
            'total_comments': int(self.df_contents.get('comment_count', 0).sum()),
            'avg_comments': float(self.df_contents.get('comment_count', 0).mean()),
        }

        if self.df_comments is not None:
            stats['total_comments_posts'] = len(self.df_comments)

        return stats

    def extract_custom(
        self,
        filter_func: Callable,
        extract_func: Callable,
        top_n: int = 20
    ) -> List[Dict[str, Any]]:
        """
        è‡ªå®šä¹‰æå–å‡½æ•°

        Args:
            filter_func: è¿‡æ»¤å‡½æ•°ï¼Œæ¥æ”¶rowï¼Œè¿”å›bool
            extract_func: æå–å‡½æ•°ï¼Œæ¥æ”¶rowï¼Œè¿”å›dict
            top_n: è¿”å›å‰Næ¡

        Returns:
            æå–çš„ç»“æœåˆ—è¡¨
        """
        results = []

        for idx, row in self.df_contents.iterrows():
            if filter_func(row):
                extracted = extract_func(row)
                if extracted:
                    results.append(extracted)

        return results[:top_n]

    def print_results(
        self,
        results: List[Dict[str, Any]],
        title: str = "æå–ç»“æœ",
        max_desc_length: int = 150
    ):
        """
        ç¾åŒ–æ‰“å°ç»“æœ

        Args:
            results: ç»“æœåˆ—è¡¨
            title: æ ‡é¢˜
            max_desc_length: æè¿°æœ€å¤§é•¿åº¦
        """
        print('\n' + '='*80)
        print(f'ğŸ“Š {title}')
        print('='*80)
        print(f'âœ… æ‰¾åˆ° {len(results)} æ¡ç»“æœ\n')

        for i, item in enumerate(results, 1):
            print(f"{i}. {item.get('title', 'N/A')[:70]}")

            # åŠ¨æ€æ˜¾ç¤ºæ‰€æœ‰å­—æ®µ
            for key, value in item.items():
                if key not in ['title', 'desc'] and value and value != 'N/A':
                    if key == 'liked':
                        print(f"   ğŸ‘ ç‚¹èµ: {value}")
                    elif key == 'price':
                        print(f"   ğŸ’° ä»·æ ¼: {value}")
                    elif key == 'matched_text':
                        print(f"   ğŸ” åŒ¹é…: {value}")
                    elif key == 'matched_keyword':
                        print(f"   ğŸ”‘ å…³é”®è¯: {value}")
                    elif key == 'collected':
                        print(f"   â­ æ”¶è—: {value}")
                    elif key == 'comment_count' or key == 'comments':
                        print(f"   ğŸ’¬ è¯„è®º: {value}")
                    else:
                        print(f"   {key}: {value}")

            if 'desc' in item and item['desc']:
                desc = item['desc']
                if len(desc) > max_desc_length:
                    desc = desc[:max_desc_length] + '...'
                print(f"   ğŸ“ {desc}")

            print('-'*80)

    def save_results(
        self,
        results: List[Dict[str, Any]],
        output_file: str,
        format: str = 'json'
    ):
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶

        Args:
            results: ç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            format: è¾“å‡ºæ ¼å¼ ('json' æˆ– 'csv')
        """
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        if format == 'json':
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        elif format == 'csv':
            df = pd.DataFrame(results)
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_file}")


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description='é€šç”¨æ•°æ®æå–å·¥å…· - ä»MediaCrawler CSVæ–‡ä»¶ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

  # 1. æå–åŒ…å«å…³é”®è¯çš„å¸–å­
  python extractor.py contents.csv --keywords "ä¸€å°ºèŠ±å›­" "æ˜Ÿå·´å…‹"

  # 2. æå–ä»·æ ¼ä¿¡æ¯
  python extractor.py contents.csv --extract-prices --top 30

  # 3. æå–åœ°ç†ä½ç½®
  python extractor.py contents.csv --extract-locations --top 15

  # 4. æå–çƒ­é—¨å¸–å­
  python extractor.py contents.csv --top-posts --top 20

  # 5. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
  python extractor.py contents.csv --pattern "(\d+)å…ƒ.*å¤©"

  # 6. ä¿å­˜ç»“æœåˆ°JSON
  python extractor.py contents.csv --keywords "å’–å•¡" --save results.json

  # 7. åŒæ—¶åŠ è½½å¸–å­+è¯„è®ºæ•°æ®
  python extractor.py contents.csv comments.csv --keywords "è‡ªä¹ "
        """
    )

    parser.add_argument('contents_file', help='å¸–å­å†…å®¹CSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('comments_file', nargs='?', help='è¯„è®ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')

    # æå–æ¨¡å¼
    extract_group = parser.add_mutually_exclusive_group()
    extract_group.add_argument('--keywords', nargs='+', help='å…³é”®è¯åˆ—è¡¨')
    extract_group.add_argument('--pattern', help='æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼')
    extract_group.add_argument('--extract-prices', action='store_true', help='æå–ä»·æ ¼ä¿¡æ¯')
    extract_group.add_argument('--extract-locations', action='store_true', help='æå–åœ°ç†ä½ç½®')
    extract_group.add_argument('--top-posts', action='store_true', help='æå–çƒ­é—¨å¸–å­')
    extract_group.add_argument('--statistics', action='store_true', help='æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯')

    # é€‰é¡¹å‚æ•°
    parser.add_argument('--top', type=int, default=20, help='è¿”å›å‰Næ¡ç»“æœï¼ˆé»˜è®¤: 20ï¼‰')
    parser.add_argument('--min-likes', type=int, default=0, help='æœ€å°ç‚¹èµæ•°è¿‡æ»¤')
    parser.add_argument('--fields', nargs='+', default=['title', 'desc'], help='æœç´¢å­—æ®µåˆ—è¡¨')
    parser.add_argument('--sort-by', default='liked_count', help='æ’åºå­—æ®µ')
    parser.add_argument('--save', help='ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ï¼ˆè‡ªåŠ¨è¯†åˆ«æ ¼å¼: .json æˆ– .csvï¼‰')
    parser.add_argument('--format', choices=['json', 'csv'], help='è¾“å‡ºæ ¼å¼')

    args = parser.parse_args()

    # åˆå§‹åŒ–æå–å™¨
    extractor = UniversalExtractor(args.contents_file, args.comments_file)

    results = None
    title = "æå–ç»“æœ"

    # æ‰§è¡Œæå–
    if args.keywords:
        results = extractor.extract_by_keywords(
            keywords=args.keywords,
            search_fields=args.fields,
            top_n=args.top,
            sort_by=args.sort_by
        )
        title = f"å…³é”®è¯æœç´¢: {', '.join(args.keywords)}"

    elif args.pattern:
        results = extractor.extract_by_pattern(
            pattern=args.pattern,
            search_fields=args.fields,
            top_n=args.top
        )
        title = f"æ­£åˆ™åŒ¹é…: {args.pattern}"

    elif args.extract_prices:
        results = extractor.extract_prices(top_n=args.top)
        title = "ä»·æ ¼ä¿¡æ¯æå–"

    elif args.extract_locations:
        results = extractor.extract_locations(top_n=args.top)
        title = "åœ°ç†ä½ç½®åˆ†å¸ƒ"
        # åœ°ç†ä½ç½®è¿”å›çš„æ˜¯dictï¼Œç‰¹æ®Šå¤„ç†
        print('\n' + '='*80)
        print(f'ğŸ“ {title}')
        print('='*80)
        for location, count in results.items():
            print(f'{location}: {count}æ¬¡')
        if args.save:
            extractor.save_results(
                [{'location': k, 'count': v} for k, v in results.items()],
                args.save,
                args.format or 'json'
            )
        return

    elif args.top_posts:
        results = extractor.extract_top_posts(top_n=args.top, min_likes=args.min_likes)
        title = f"çƒ­é—¨å¸–å­ TOP {args.top}"

    elif args.statistics:
        stats = extractor.extract_statistics()
        print('\n' + '='*80)
        print('ğŸ“ˆ æ•°æ®ç»Ÿè®¡ä¿¡æ¯')
        print('='*80)
        for key, value in stats.items():
            print(f'{key}: {value}')
        return

    else:
        parser.print_help()
        return

    # æ˜¾ç¤ºç»“æœ
    if results:
        extractor.print_results(results, title=title)

        # ä¿å­˜ç»“æœ
        if args.save:
            output_format = args.format
            if not output_format:
                # æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨è¯†åˆ«
                if args.save.endswith('.csv'):
                    output_format = 'csv'
                else:
                    output_format = 'json'

            extractor.save_results(results, args.save, output_format)


if __name__ == '__main__':
    main()
