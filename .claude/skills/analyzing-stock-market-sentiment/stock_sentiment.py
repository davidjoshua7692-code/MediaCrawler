"""
è‚¡å¸‚æƒ…ç»ªåˆ†æå™¨
ä¸“é—¨é’ˆå¯¹è‚¡ç¥¨è®¨è®ºè¿›è¡Œå¤šç©ºæƒ…ç»ªåˆ†æ
æ”¯æŒå°çº¢ä¹¦ã€å¾®åšã€è‚¡å§ç­‰å¹³å°æ•°æ®

æ”¯æŒä¸¤ç§åˆ†ææ–¹æ³•ï¼š
1. å…³é”®è¯åŒ¹é…ï¼ˆå¿«é€Ÿï¼Œé»˜è®¤ï¼‰
2. FinBERTï¼ˆç²¾å‡†ï¼Œéœ€ä¸‹è½½æ¨¡å‹ï¼‰
"""
import pandas as pd
import re
from collections import Counter
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import sys
import argparse


# ============================================================================
# è‚¡å¸‚ä¸“å±å…³é”®è¯é…ç½®
# ============================================================================

# å¤šç©ºå…³é”®è¯
BULLISH_KEYWORDS = [
    'æ¶¨', 'åŠ ä»“', 'ä¹°å…¥', 'çœ‹å¤š', 'èµ·é£', 'çªç ´', 'ç‰›å¸‚', 'å‘ä¸Š',
    'æŒæœ‰', 'ä¸å–', 'ç»§ç»­æ¶¨', 'è¿˜èƒ½æ¶¨', 'ç›®æ ‡', 'å¥½', 'ç‰›', 'å¼º',
    'ç¨³', 'å€¼', 'ä½å¸', 'è¡¥ä»“', 'æœºä¼š', 'ä¹°', 'æŒä»“', 'æ‹¿ä½',
    'çœ‹å¥½', 'å€¼å¾—', 'ä»·å€¼', 'ä¼˜ç§€', 'é¾™å¤´', 'ä¸Šæ¶¨', 'æ”€å‡'
]

BEARISH_KEYWORDS = [
    'è·Œ', 'å‡ä»“', 'å–å‡º', 'çœ‹ç©º', 'å›è°ƒ', 'ç†Šå¸‚', 'å‘ä¸‹',
    'å‡ºè´§', 'é«˜ä¼°', 'è´µ', 'å¼±', 'é£é™©', 'æ€•', 'è·Œäº†', 'æ¸…ä»“',
    'å‰²è‚‰', 'äºæŸ', 'å¥—', 'æ€•è·Œ', 'è¿˜ä¼šè·Œ', 'æ´—ç›˜', 'å±é™©',
    'æ‹…å¿ƒ', 'æ€•è¢«å¥—', 'æ­¢æŸ', 'ç¦»åœº', 'é€ƒé¡¶', 'æ³¡æ²«'
]

NEUTRAL_KEYWORDS = [
    'è§‚æœ›', 'ç­‰å¾…', 'å†çœ‹çœ‹', 'ä¸ç¡®å®š', 'éœ‡è¡', 'æ¨ªç›˜',
    'æ•´ç†', 'çŠ¹è±«', 'æš‚æ—¶ä¸åŠ¨'
]

# æŠ•èµ„è¡Œä¸ºå…³é”®è¯
BEHAVIOR_KEYWORDS = {
    'åŠ ä»“/ä¹°å…¥': ['åŠ ä»“', 'ä¹°å…¥', 'ä¹°äº†', 'è¡¥ä»“', 'æŠ„åº•', 'å»ºä»“', 'ä¸Šè½¦'],
    'å‡ä»“/å–å‡º': ['å‡ä»“', 'å–å‡º', 'å–äº†', 'æ¸…ä»“', 'æ­¢ç›ˆ', 'å‰²è‚‰', 'è·‘äº†', 'ä¸‹è½¦'],
    'æŒæœ‰/è§‚æœ›': ['æŒæœ‰', 'æ‹¿ä½', 'ä¸åŠ¨', 'è§‚æœ›', 'ç­‰å¾…', 'èººå¹³', 'é”ä»“']
}

# å…³æ³¨ä¸»é¢˜å…³é”®è¯
THEME_KEYWORDS = {
    'é»„é‡‘': ['é»„é‡‘', 'é‡‘', 'è´µé‡‘å±', 'é‡‘ä»·'],
    'é“œä»·': ['é“œ', 'é“œä»·', 'lme', 'æœ‰è‰²', 'å·¥ä¸šé‡‘å±'],
    'ä¸šç»©/è´¢æŠ¥': ['ä¸šç»©', 'è´¢æŠ¥', 'åˆ©æ¶¦', 'è¥æ”¶', 'å¹´æŠ¥', 'ä¸­æŠ¥', 'å­£æŠ¥', 'roe'],
    'ä¼°å€¼': ['ä¼°å€¼', 'å¸‚ç›ˆç‡', 'pe', 'è´µäº†', 'ä¾¿å®œ', 'é«˜ä¼°', 'ä½ä¼°', 'æ³¡æ²«'],
    'åˆ†çº¢': ['åˆ†çº¢', 'è‚¡æ¯', 'æ´¾æ¯', 'è‚¡æ¯ç‡'],
    'é”‚çŸ¿': ['é”‚', 'é”‚çŸ¿', 'ç¢³é…¸é”‚', 'é”‚èµ„æº'],
    'å®è§‚ç»æµ': ['ç¾è”å‚¨', 'é™æ¯', 'åˆ©ç‡', 'ç¾å…ƒ', 'å®è§‚', 'ç»æµ', 'é€šèƒ€'],
    'æŠ€æœ¯é¢': ['æ”¯æ’‘', 'å‹åŠ›', 'é˜»åŠ›', 'çªç ´', 'è¶‹åŠ¿', 'éœ‡è¡', 'å‡çº¿', 'macd', 'kçº¿']
}

# é£é™©ä¿¡å·å…³é”®è¯
RISK_SIGNALS = {
    'æƒ…ç»ªè¿‡çƒ­': ['ä»ä¸å¥—äºº', 'åªä¼šæ¶¨', 'é—­çœ¼ä¹°', 'ç¨³èµš', 'è‚¯å®šæ¶¨', 'æ— è„‘ä¹°'],
    'é«˜ä½éœ‡è¡': ['ä¸æ˜¯èˆ’æœçš„ä¸Šè½¦ç‚¹', 'ç­‰å›è°ƒ', 'è§‚æœ›ä¸€ä¸‹', 'å†çœ‹çœ‹'],
    'è·åˆ©å›å': ['è·åˆ©äº†ç»“', 'è½è¢‹ä¸ºå®‰', 'å…ˆå‡ºæ¥', 'çŸ­çº¿èµ„é‡‘'],
    'FOMOæƒ…ç»ª': ['å–é£', 'ä¹°å°‘', 'åæ‚”', 'é”™è¿‡', 'æ²¡ä¹°']
}


# ============================================================================
# FinBERT é›†æˆ
# ============================================================================

# å°è¯•å¯¼å…¥ FinBERT åˆ†æå™¨
try:
    from finbert_analyzer import HybridSentimentAnalyzer
    FINBERT_AVAILABLE = True
except ImportError:
    FINBERT_AVAILABLE = False

# å…¨å±€ FinBERT åˆ†æå™¨
_finbert_analyzer = None


def get_project_paths():
    """
    è·å–é¡¹ç›®è·¯å¾„ï¼ˆé”šå®šåˆ°.claudeæ–‡ä»¶å¤¹ï¼‰

    Returns:
        dict: {
            'project_root': é¡¹ç›®æ ¹ç›®å½•,
            'data_dir': æ•°æ®ç›®å½•,
            'model_dir': æ¨¡å‹ç›®å½•,
            'report_dir': æŠ¥å‘Šç›®å½•ï¼ˆé¡¹ç›®æ ¹ç›®å½•/REPORTï¼‰
        }
    """
    script_dir = Path(__file__).parent
    # .claude/skills/analyzing-stock-market-sentiment/ -> .claude/
    claude_dir = script_dir.parent.parent
    # .claude/ -> é¡¹ç›®æ ¹ç›®å½•
    project_root = claude_dir.parent

    return {
        'project_root': project_root,
        'data_dir': project_root / "data" / "xhs" / "csv",
        'model_dir': project_root / "models" / "finbert_chinese",
        'report_dir': project_root / "REPORT"  # æ”¹ä¸ºé¡¹ç›®æ ¹ç›®å½•/REPORT
    }


def get_finbert_analyzer():
    """è·å– FinBERT åˆ†æå™¨ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _finbert_analyzer

    if not FINBERT_AVAILABLE:
        return None

    if _finbert_analyzer is None:
        try:
            paths = get_project_paths()
            _finbert_analyzer = HybridSentimentAnalyzer(
                model_path=str(paths['model_dir'])
            )
            if _finbert_analyzer.finbert.model_loaded:
                print("âœ… FinBERT æ¨¡å‹å·²å¯ç”¨ï¼ˆæ··åˆæ¨¡å¼ï¼‰")
        except Exception as e:
            print(f"âš ï¸  FinBERT åˆå§‹åŒ–å¤±è´¥: {e}")
            print("   å°†ä½¿ç”¨çº¯å…³é”®è¯åŒ¹é…æ¨¡å¼")
            _finbert_analyzer = None

    return _finbert_analyzer


# ============================================================================
# æ ¸å¿ƒåˆ†æå‡½æ•°
# ============================================================================

def analyze_sentiment(text: str, use_finbert: bool = True) -> Tuple[str, int]:
    """
    åˆ†æå•æ¡æ–‡æœ¬çš„æƒ…ç»ª

    Args:
        text: æ–‡æœ¬å†…å®¹
        use_finbert: æ˜¯å¦å°è¯•ä½¿ç”¨ FinBERTï¼ˆé»˜è®¤Trueï¼‰

    Returns:
        (æƒ…ç»ªç±»å‹, å¾—åˆ†) - æƒ…ç»ªç±»å‹ä¸º 'bullish', 'bearish', 'neutral', 'uncertain'
    """
    if pd.isna(text):
        return 'uncertain', 0

    # ä¼˜å…ˆä½¿ç”¨ FinBERTï¼ˆå¦‚æœå¯ç”¨ä¸”å¯ç”¨ï¼‰
    if use_finbert and FINBERT_AVAILABLE:
        finbert = get_finbert_analyzer()
        if finbert and finbert.finbert.model_loaded:
            try:
                result = finbert.analyze(str(text))
                # è¿”å›å®Œæ•´ç»“æœï¼ŒåŒ…å«ç»†ç²’åº¦æƒ…ç»ª
                return result
            except Exception as e:
                # FinBERT å¤±è´¥ï¼Œå›é€€åˆ°å…³é”®è¯
                pass

    # å…³é”®è¯åŒ¹é…ï¼ˆå›é€€æ–¹æ¡ˆæˆ–é»˜è®¤æ–¹æ¡ˆï¼‰
    text_lower = str(text).lower()

    bullish_score = sum(1 for kw in BULLISH_KEYWORDS if kw in text_lower)
    bearish_score = sum(1 for kw in BEARISH_KEYWORDS if kw in text_lower)
    neutral_score = sum(1 for kw in NEUTRAL_KEYWORDS if kw in text_lower)

    if bullish_score > bearish_score and bullish_score > neutral_score:
        return 'bullish', bullish_score
    elif bearish_score > bullish_score and bearish_score > neutral_score:
        return 'bearish', bearish_score
    elif neutral_score > 0:
        return 'neutral', neutral_score
    else:
        return 'uncertain', 0


def extract_price_targets(text: str) -> List[float]:
    """
    æå–æ–‡æœ¬ä¸­çš„ä»·æ ¼ç›®æ ‡

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        ä»·æ ¼ç›®æ ‡åˆ—è¡¨
    """
    if pd.isna(text):
        return []

    # åŒ¹é…æ•°å­—ï¼Œåˆç†è‚¡ä»·èŒƒå›´ 5-200å…ƒ
    price_pattern = r'(\d{1,3}\.?\d*)\s*[å…ƒå—]?'
    prices = []

    for match in re.finditer(price_pattern, str(text)):
        try:
            price = float(match.group(1))
            # è¿‡æ»¤åˆç†è‚¡ä»·èŒƒå›´
            if 5 <= price <= 200:
                # æ’é™¤æ˜æ˜¾ä¸æ˜¯è‚¡ä»·çš„æ•°å­—ï¼ˆå¦‚100è‚¡ã€10å¹´ç­‰ï¼‰
                if not any(exclude in str(text) for exclude in ['è‚¡', 'å¹´', 'å€', '%', 'æ¬¡']):
                    prices.append(price)
        except (ValueError, IndexError):
            continue

    return prices


def analyze_investment_behavior(text: str) -> Optional[str]:
    """
    è¯†åˆ«æŠ•èµ„è¡Œä¸º

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        è¡Œä¸ºç±»å‹æˆ–None
    """
    if pd.isna(text):
        return None

    text_lower = str(text).lower()

    for behavior, keywords in BEHAVIOR_KEYWORDS.items():
        if any(kw in text_lower for kw in keywords):
            return behavior

    return None


def detect_themes(text: str) -> List[str]:
    """
    æ£€æµ‹æ–‡æœ¬ä¸­çš„æŠ•èµ„ä¸»é¢˜

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        ä¸»é¢˜åˆ—è¡¨
    """
    if pd.isna(text):
        return []

    text_lower = str(text).lower()
    detected_themes = []

    for theme, keywords in THEME_KEYWORDS.items():
        if any(kw in text_lower for kw in keywords):
            detected_themes.append(theme)

    return detected_themes


def detect_risk_signals(text: str) -> List[str]:
    """
    æ£€æµ‹é£é™©ä¿¡å·

    Args:
        text: æ–‡æœ¬å†…å®¹

    Returns:
        é£é™©ä¿¡å·åˆ—è¡¨
    """
    if pd.isna(text):
        return []

    text_lower = str(text).lower()
    signals = []

    for signal_type, keywords in RISK_SIGNALS.items():
        if any(kw in text_lower for kw in keywords):
            signals.append(signal_type)

    return signals


# ============================================================================
# ä¸»åˆ†æå‡½æ•°
# ============================================================================

def analyze_stock_sentiment(
    comments_file: str,
    contents_file: str = None,
    stock_name: str = 'ç›®æ ‡è‚¡ç¥¨',
    output_dir: str = None,
    use_finbert: bool = True
) -> Dict:
    """
    ç»¼åˆåˆ†æè‚¡ç¥¨è®¨è®ºæƒ…ç»ª

    Args:
        comments_file: è¯„è®ºCSVæ–‡ä»¶è·¯å¾„
        contents_file: å†…å®¹CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        stock_name: è‚¡ç¥¨åç§°
        output_dir: è¾“å‡ºç›®å½•
        use_finbert: æ˜¯å¦ä½¿ç”¨ FinBERTï¼ˆé»˜è®¤Trueï¼Œè‡ªåŠ¨å›é€€åˆ°å…³é”®è¯ï¼‰

    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“Š {stock_name} - è‚¡å¸‚æƒ…ç»ªåˆ†ææŠ¥å‘Š")
    print(f"{'='*80}\n")

    # è¯»å–æ•°æ®
    df_comments = pd.read_csv(comments_file)
    df_contents = pd.read_csv(contents_file) if contents_file else None

    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
    print(f"   è¯„è®ºæ•°æ®: {len(df_comments)} æ¡")
    if df_contents is not None:
        print(f"   å†…å®¹æ•°æ®: {len(df_contents)} æ¡")

    # 1. å¤šç©ºæƒ…ç»ªåˆ†æ
    print(f"\n{'='*80}")
    print(f"ğŸ“ˆ ä¸€ã€å¤šç©ºæƒ…ç»ªåˆ†å¸ƒ")
    print(f"{'='*80}\n")

    bullish_comments = []
    bearish_comments = []
    neutral_comments = []
    uncertain_comments = []
    fine_grained_stats = Counter()  # ç»†ç²’åº¦æƒ…ç»ªç»Ÿè®¡
    layer_stats = Counter({'ç¬¬1å±‚(å…³é”®è¯æ˜ç¡®)': 0, 'ç¬¬2å±‚(FinBERT)': 0})  # åˆ†å±‚ç»Ÿè®¡

    for idx, row in df_comments.iterrows():
        content = row.get('content', '')
        result = analyze_sentiment(content, use_finbert)

        # å…¼å®¹è¿”å›å€¼ï¼šå¯èƒ½æ˜¯ (sentiment, score) æˆ– dict
        if isinstance(result, dict):
            sentiment = result['sentiment']
            score = result['confidence'] * 10  # è½¬æ¢ä¸ºå¾—åˆ†
            fine_grained = result.get('fine_grained', None)
            method = result.get('method', 'unknown')

            # ç»Ÿè®¡åˆ†å±‚
            if method == 'keyword':
                layer_stats['ç¬¬1å±‚(å…³é”®è¯æ˜ç¡®)'] += 1
            elif method == 'finbert':
                layer_stats['ç¬¬2å±‚(FinBERT)'] += 1

            # FinBERT ç»“æœç›´æ¥çº³å…¥ç»†ç²’åº¦ç»Ÿè®¡
            if fine_grained:
                fine_grained_stats[fine_grained] += 1

            # å°† FinBERT ç»“æœåŠ å…¥åˆ†ç±»åˆ—è¡¨
            if sentiment == 'bullish':
                bullish_comments.append((content, score, row.get('like_count', 0), row.get('ip_location', '')))
            elif sentiment == 'bearish':
                bearish_comments.append((content, score, row.get('like_count', 0), row.get('ip_location', '')))
            elif sentiment == 'neutral':
                neutral_comments.append((content, score, row.get('like_count', 0), row.get('ip_location', '')))
            else:
                uncertain_comments.append(content)

        else:
            # å…³é”®è¯æ¨¡å¼ï¼šéœ€è¦é‡æ–°è®¡ç®—å¾—åˆ†å·®å¹¶æ˜ å°„åˆ°ç»†ç²’åº¦æƒ…ç»ª
            sentiment, score = result

            # é‡æ–°è®¡ç®—å…³é”®è¯å¾—åˆ†
            text_lower = str(content).lower()
            bullish_score = sum(1 for kw in BULLISH_KEYWORDS if kw in text_lower)
            bearish_score = sum(1 for kw in BEARISH_KEYWORDS if kw in text_lower)
            score_diff = abs(bullish_score - bearish_score)

            # åªç»Ÿè®¡å¾—åˆ†å·® â‰¥ 2 çš„æ˜ç¡®è¯„è®º
            if score_diff >= 2:
                layer_stats['ç¬¬1å±‚(å…³é”®è¯æ˜ç¡®)'] += 1

                # æ˜ å°„åˆ°ç»†ç²’åº¦æƒ…ç»ª
                if sentiment == 'bullish':
                    if score_diff >= 4:
                        fine_grained = 'å¼ºçƒˆçœ‹æ¶¨ğŸ“ˆğŸ“ˆ'
                    else:  # score_diff = 2-3
                        fine_grained = 'çœ‹æ¶¨ğŸ“ˆ'
                elif sentiment == 'bearish':
                    if score_diff >= 4:
                        fine_grained = 'å¼ºçƒˆçœ‹è·ŒğŸ“‰ğŸ“‰'
                    else:  # score_diff = 2-3
                        fine_grained = 'çœ‹è·ŒğŸ“‰'
                else:  # neutral
                    fine_grained = 'çº¯ä¸­æ€§âšª'

                if fine_grained:
                    fine_grained_stats[fine_grained] += 1

                # åªå°†æ˜ç¡®è¯„è®º(score_diff >= 2)åŠ å…¥åˆ†ç±»åˆ—è¡¨
                if sentiment == 'bullish':
                    bullish_comments.append((content, score, row.get('like_count', 0), row.get('ip_location', '')))
                elif sentiment == 'bearish':
                    bearish_comments.append((content, score, row.get('like_count', 0), row.get('ip_location', '')))
                elif sentiment == 'neutral':
                    neutral_comments.append((content, score, row.get('like_count', 0), row.get('ip_location', '')))
            # score_diff < 2 çš„æ¨¡ç³Šè¯„è®ºï¼šåœ¨å…³é”®è¯æ¨¡å¼ä¸‹è·³è¿‡ï¼Œä¸åŠ å…¥ä»»ä½•ç»Ÿè®¡
            # (è¿™äº›è¯„è®ºåº”è¯¥ç”±FinBERTç¬¬2å±‚å¤„ç†ï¼Œä½†use_finbert=Falseæ—¶æ²¡æœ‰ç¬¬2å±‚)

    total_classified = len(bullish_comments) + len(bearish_comments) + len(neutral_comments)

    if total_classified > 0:
        bullish_pct = len(bullish_comments) / total_classified * 100
        bearish_pct = len(bearish_comments) / total_classified * 100
        neutral_pct = len(neutral_comments) / total_classified * 100
        net_sentiment = bullish_pct - bearish_pct

        print(f"  çœ‹æ¶¨ï¼ˆå¤šå¤´ï¼‰: {len(bullish_comments)} æ¡ ({bullish_pct:.1f}%)")
        print(f"  çœ‹è·Œï¼ˆç©ºå¤´ï¼‰: {len(bearish_comments)} æ¡ ({bearish_pct:.1f}%)")
        print(f"  è§‚æœ›ï¼ˆä¸­æ€§ï¼‰: {len(neutral_comments)} æ¡ ({neutral_pct:.1f}%)")
        print(f"  æœªæ˜ç¡®: {len(uncertain_comments)} æ¡")
        print(f"\n  ğŸ¯ å‡€å¤šå¤´æƒ…ç»ª: {net_sentiment:+.1f}%")

        # æ˜¾ç¤ºåˆ†å±‚ç»Ÿè®¡
        total_processed = sum(layer_stats.values())
        if total_processed > 0:
            print(f"\n  ğŸ“Š åˆ†æåˆ†å±‚ç»Ÿè®¡:")
            for layer, count in layer_stats.most_common():
                pct = count / total_processed * 100
                print(f"    {layer}: {count} æ¡ ({pct:.1f}%)")

        # æ˜¾ç¤ºç»†ç²’åº¦æƒ…ç»ªåˆ†å¸ƒ
        if fine_grained_stats:
            print(f"\n  ğŸ“Š ç»†ç²’åº¦æƒ…ç»ªåˆ†å¸ƒ (9ç±»):")
            total_fine_grained = sum(fine_grained_stats.values())
            for emotion, count in fine_grained_stats.most_common():
                pct = count / total_fine_grained * 100
                print(f"    {emotion}: {count} æ¡ ({pct:.1f}%)")

        # åˆ¤æ–­æƒ…ç»ªåŒºé—´
        if net_sentiment > 50:
            sentiment_level = "ğŸ”´ æåº¦è´ªå©ªï¼ˆé£é™©è­¦å‘Šï¼‰"
        elif net_sentiment > 30:
            sentiment_level = "ğŸŸ  è´ªå©ªï¼ˆéœ€è°¨æ…ï¼‰"
        elif net_sentiment > 10:
            sentiment_level = "ğŸŸ¢ é€‚åº¦çœ‹å¤šï¼ˆå¥åº·ï¼‰"
        elif net_sentiment > -10:
            sentiment_level = "âšª ä¸­æ€§ï¼ˆè§‚æœ›ï¼‰"
        elif net_sentiment > -30:
            sentiment_level = "ğŸ”µ é€‚åº¦çœ‹ç©ºï¼ˆè°¨æ…ï¼‰"
        else:
            sentiment_level = "âš« æåº¦ææƒ§ï¼ˆæœºä¼šåŒºé—´ï¼‰"

        print(f"\n  æƒ…ç»ªåŒºé—´: {sentiment_level}")

    # 2. ä»·æ ¼ç›®æ ‡åˆ†æ
    print(f"\n{'='*80}")
    print(f"ğŸ’° äºŒã€ä»·æ ¼é¢„æœŸåˆ†æ")
    print(f"{'='*80}\n")

    all_price_targets = []
    for idx, row in df_comments.iterrows():
        content = row.get('content', '')
        prices = extract_price_targets(content)
        for price in prices:
            all_price_targets.append((content, price, row.get('like_count', 0)))

    if all_price_targets:
        prices_only = [p[1] for p in all_price_targets]
        print(f"  æåŠä»·æ ¼ç›®æ ‡: {len(all_price_targets)} æ¬¡")
        print(f"  ä»·æ ¼åŒºé—´: {min(prices_only):.2f} - {max(prices_only):.2f} å…ƒ")
        print(f"  å¹³å‡é¢„æœŸ: {sum(prices_only)/len(prices_only):.2f} å…ƒ")

        # ä»·æ ¼é¢‘æ¬¡ç»Ÿè®¡
        price_counter = Counter(prices_only)
        top_prices = price_counter.most_common(10)

        print(f"\n  çƒ­é—¨ç›®æ ‡ä»·ä½ Top 10:")
        for price, count in top_prices:
            # è®¡ç®—æ”¯æŒåº¦ï¼ˆç‚¹èµæ•°ï¼‰
            related_comments = [c for c in all_price_targets if abs(c[1] - price) < 0.01]
            total_likes = sum(c[2] for c in related_comments)
            print(f"    {price:6.2f} å…ƒ: {count:2d}æ¬¡æåŠ | ğŸ‘{total_likes} æ”¯æŒ")

    # 3. æŠ•èµ„è¡Œä¸ºåˆ†æ
    print(f"\n{'='*80}")
    print(f"ğŸ¯ ä¸‰ã€æŠ•èµ„è€…è¡Œä¸ºåˆ†æ")
    print(f"{'='*80}\n")

    behavior_stats = {behavior: 0 for behavior in BEHAVIOR_KEYWORDS.keys()}
    for idx, row in df_comments.iterrows():
        content = row.get('content', '')
        behavior = analyze_investment_behavior(content)
        if behavior:
            behavior_stats[behavior] += 1

    for behavior, count in behavior_stats.items():
        if count > 0:
            print(f"  {behavior}: {count} æ¡è¯„è®º")

    # 4. æ ¸å¿ƒå…³æ³¨ä¸»é¢˜
    print(f"\n{'='*80}")
    print(f"ğŸ” å››ã€æ ¸å¿ƒå…³æ³¨ä¸»é¢˜")
    print(f"{'='*80}\n")

    theme_counter = Counter()
    for idx, row in df_comments.iterrows():
        content = row.get('content', '')
        themes = detect_themes(content)
        theme_counter.update(themes)

    if theme_counter:
        print(f"  ä¸»é¢˜æåŠæ’å:")
        for theme, count in theme_counter.most_common():
            print(f"    {theme}: {count} æ¡æåŠ")

    # 5. çœ‹æ¶¨ç†ç”± Top 10
    print(f"\n{'='*80}")
    print(f"âœ… äº”ã€çœ‹æ¶¨ç†ç”± Top 10ï¼ˆæŒ‰ç‚¹èµæ’åºï¼‰")
    print(f"{'='*80}\n")

    bullish_comments_sorted = sorted(bullish_comments, key=lambda x: x[2], reverse=True)
    for i, (content, score, likes, location) in enumerate(bullish_comments_sorted[:10], 1):
        display_content = content[:80] + '...' if len(content) > 80 else content
        print(f"{i:2d}. [{location}] ğŸ‘{likes}: {display_content}")

    # 6. çœ‹è·Œ/æ‹…å¿§ç†ç”± Top 10
    print(f"\n{'='*80}")
    print(f"âš ï¸  å…­ã€çœ‹è·Œ/æ‹…å¿§ç†ç”± Top 10")
    print(f"{'='*80}\n")

    bearish_comments_sorted = sorted(bearish_comments, key=lambda x: x[2], reverse=True)
    for i, (content, score, likes, location) in enumerate(bearish_comments_sorted[:10], 1):
        display_content = content[:80] + '...' if len(content) > 80 else content
        print(f"{i:2d}. [{location}] ğŸ‘{likes}: {display_content}")

    # 7. æŠ•èµ„è€…æ•…äº‹
    print(f"\n{'='*80}")
    print(f"ğŸ“– ä¸ƒã€æŠ•èµ„è€…æ•…äº‹ä¸æ“ä½œ")
    print(f"{'='*80}\n")

    stories = []
    for idx, row in df_comments.iterrows():
        content = str(row.get('content', ''))
        if pd.notna(content) and any(kw in content for kw in ['ä¹°äº†', 'å–äº†', 'å–é£', 'åæ‚”', 'å¯æƒœ', 'åº†å¹¸', 'æŒæœ‰', 'å¹´']):
            likes = row.get('like_count', 0)
            if likes and likes > 5:  # åªå–é«˜äº’åŠ¨æ•…äº‹
                stories.append((content, likes, row.get('ip_location', '')))

    stories_sorted = sorted(stories, key=lambda x: x[1], reverse=True)
    for i, (content, likes, location) in enumerate(stories_sorted[:8], 1):
        display_content = content[:100] + '...' if len(content) > 100 else content
        print(f"{i}. [{location}] ğŸ‘{likes}: {display_content}")

    # 8. é£é™©ä¿¡å·è¯†åˆ«
    print(f"\n{'='*80}")
    print(f"ğŸš¨ å…«ã€é£é™©ä¿¡å·è¯†åˆ«")
    print(f"{'='*80}\n")

    risk_signals_found = Counter()
    risk_examples = {signal: [] for signal in RISK_SIGNALS.keys()}

    for idx, row in df_comments.iterrows():
        content = row.get('content', '')
        signals = detect_risk_signals(content)
        for signal in signals:
            risk_signals_found[signal] += 1
            if len(risk_examples[signal]) < 3:  # æ¯ç±»ä¿¡å·ä¿ç•™3ä¸ªä¾‹å­
                risk_examples[signal].append(content[:60])

    if risk_signals_found:
        print(f"  æ£€æµ‹åˆ°é£é™©ä¿¡å·:")
        for signal, count in risk_signals_found.most_common():
            print(f"\n  âš ï¸  {signal}: {count} æ¡æåŠ")
            for example in risk_examples[signal]:
                print(f"     - {example}...")
    else:
        print("  æœªæ£€æµ‹åˆ°æ˜æ˜¾é£é™©ä¿¡å·")

    # 9. ç»¼åˆæŠ•èµ„å»ºè®®
    print(f"\n{'='*80}")
    print(f"ğŸ’¡ ä¹ã€ç»¼åˆæŠ•èµ„æ´å¯Ÿ")
    print(f"{'='*80}\n")

    insights = []

    # æƒ…ç»ªé¢
    if total_classified > 0:
        if net_sentiment > 50:
            insights.append("âš ï¸  æƒ…ç»ªè¿‡çƒ­ï¼šå‡€å¤šå¤´è¶…è¿‡50%ï¼Œéœ€è­¦æƒ•çŸ­æœŸå›è°ƒé£é™©")
        elif net_sentiment > 30:
            insights.append("âš ï¸  æƒ…ç»ªåçƒ­ï¼šå»ºè®®å…³æ³¨è·åˆ©å›åå‹åŠ›")
        elif net_sentiment > 10:
            insights.append("âœ… æƒ…ç»ªå¥åº·ï¼šå¤šå¤´å ä¼˜ï¼Œå¸‚åœºä¿¡å¿ƒè¾ƒå¼º")
        elif net_sentiment > -10:
            insights.append("âšª æƒ…ç»ªä¸­æ€§ï¼šå¤šç©ºåˆ†æ­§ï¼Œç­‰å¾…æ–¹å‘é€‰æ‹©")
        else:
            insights.append("ğŸ’¡ æƒ…ç»ªåç©ºï¼šå¯èƒ½å­˜åœ¨æœºä¼šåŒºé—´")

    # ä»·æ ¼é¢
    if all_price_targets:
        avg_price = sum(p[1] for p in all_price_targets) / len(all_price_targets)
        insights.append(f"ğŸ’° ä»·æ ¼å…±è¯†ï¼šå¸‚åœºå¹³å‡ç›®æ ‡ä»· {avg_price:.2f} å…ƒ")

    # è¡Œä¸ºé¢
    total_behavior = sum(behavior_stats.values())
    if total_behavior > 0:
        buy_ratio = behavior_stats.get('åŠ ä»“/ä¹°å…¥', 0) / total_behavior * 100
        if buy_ratio > 60:
            insights.append(f"ğŸ“ˆ ä¹°å…¥æ„æ„¿å¼ºï¼š{buy_ratio:.1f}% æŠ•èµ„è€…è®¡åˆ’åŠ ä»“")
        elif buy_ratio < 40:
            insights.append(f"ğŸ“‰ å–å‡ºå‹åŠ›å¢ï¼š{buy_ratio:.1f}% æŠ•èµ„è€…è®¡åˆ’ä¹°å…¥")

    # é£é™©é¢
    if risk_signals_found:
        top_risk = risk_signals_found.most_common(1)[0]
        insights.append(f"ğŸš¨ é£é™©æç¤ºï¼šæ£€æµ‹åˆ°'{top_risk[0]}'ä¿¡å· {top_risk[1]} æ¬¡")

    for i, insight in enumerate(insights, 1):
        print(f"  {i}. {insight}")

    print(f"\n{'='*80}")
    print(f"âœ… åˆ†æå®Œæˆ!")
    print(f"{'='*80}\n")

    # è¿”å›ç»“æœ
    return {
        'total_comments': len(df_comments),
        'bullish_count': len(bullish_comments),
        'bearish_count': len(bearish_comments),
        'neutral_count': len(neutral_comments),
        'bullish_pct': bullish_pct if total_classified > 0 else 0,
        'bearish_pct': bearish_pct if total_classified > 0 else 0,
        'net_sentiment': net_sentiment if total_classified > 0 else 0,
        'price_targets': all_price_targets,
        'behavior_stats': behavior_stats,
        'theme_stats': dict(theme_counter),
        'risk_signals': dict(risk_signals_found)
    }


# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

def find_latest_dedup_files(data_dir: str = None) -> Tuple[Optional[str], Optional[str]]:
    """
    è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°çš„å»é‡CSVæ–‡ä»¶

    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨æŸ¥æ‰¾ï¼‰

    Returns:
        (comments_dedup_file, contents_dedup_file) æ‰¾åˆ°çš„æ–‡ä»¶è·¯å¾„ï¼Œæœªæ‰¾åˆ°è¿”å›None
    """
    if data_dir is None:
        paths = get_project_paths()
        data_dir = paths['data_dir']
    else:
        data_dir = Path(data_dir)

    if not data_dir.exists():
        return None, None

    # æŸ¥æ‰¾ -dedup åç¼€çš„æ–‡ä»¶ï¼ˆæ”¯æŒ search_comments_2026-01-20-dedup.csv æ ¼å¼ï¼‰
    comments_dedup_files = list(data_dir.glob("*comments*dedup.csv"))
    contents_dedup_files = list(data_dir.glob("*contents*dedup.csv"))

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œå–æœ€æ–°çš„
    comments_dedup = max(comments_dedup_files, key=lambda f: f.stat().st_mtime) if comments_dedup_files else None
    contents_dedup = max(contents_dedup_files, key=lambda f: f.stat().st_mtime) if contents_dedup_files else None

    return comments_dedup, contents_dedup


def save_report_to_file(report_content: str, stock_name: str, output_dir: str = None, suffix: str = ""):
    """
    ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ï¼ˆè¾“å‡ºåˆ°é¡¹ç›®æ ¹ç›®å½•/REPORT/ï¼‰

    Args:
        report_content: æŠ¥å‘Šå†…å®¹
        stock_name: è‚¡ç¥¨åç§°
        output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤é¡¹ç›®æ ¹ç›®å½•/REPORT/ï¼‰
        suffix: æ–‡ä»¶ååç¼€ï¼ˆç”¨äºåŒºåˆ†ä¸åŒæŠ¥å‘Šï¼‰
    """
    from datetime import datetime

    if output_dir is None:
        paths = get_project_paths()
        output_path = paths['report_dir']
    else:
        output_path = Path(output_dir)

    output_path.mkdir(parents=True, exist_ok=True)

    # ç”Ÿæˆæ–‡ä»¶åï¼šè‚¡ç¥¨å_æ—¥æœŸæ—¶é—´_åç¼€.txt
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{stock_name}_æƒ…ç»ªåˆ†æ_{timestamp}{suffix}.txt"
    file_path = output_path / filename

    with open(file_path, 'w', encoding='utf-8') as f:
        f.write(report_content)

    print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {file_path}")
    return str(file_path)


if __name__ == "__main__":
    import sys
    import argparse
    from io import StringIO

    parser = argparse.ArgumentParser(
        description='è‚¡å¸‚æƒ…ç»ªåˆ†æå™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # è‡ªåŠ¨æ¨¡å¼ï¼šæŸ¥æ‰¾æœ€æ–°å»é‡æ–‡ä»¶ï¼Œè¾“å‡ºåˆ° REPORT/
  python stock_sentiment.py --auto

  # æ‰‹åŠ¨æŒ‡å®šæ–‡ä»¶
  python stock_sentiment.py data/comments-dedup.csv data/contents-dedup.csv "ç´«é‡‘çŸ¿ä¸š"

  # ç¦ç”¨ FinBERT
  python stock_sentiment.py --auto --no-finbert
        """
    )

    parser.add_argument('--auto', action='store_true',
                        help='è‡ªåŠ¨æ¨¡å¼ï¼šæŸ¥æ‰¾æœ€æ–°å»é‡æ–‡ä»¶')
    parser.add_argument('--data-dir', type=str, default=None,
                        help='æ•°æ®ç›®å½•ï¼ˆé»˜è®¤: è‡ªåŠ¨ä»é¡¹ç›®æ ¹ç›®å½•æŸ¥æ‰¾ï¼‰')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='æŠ¥å‘Šè¾“å‡ºç›®å½•ï¼ˆé»˜è®¤: é¡¹ç›®æ ¹ç›®å½•/REPORT/ï¼‰')
    parser.add_argument('comments_file', nargs='?', help='è¯„è®ºCSVæ–‡ä»¶è·¯å¾„')
    parser.add_argument('contents_file', nargs='?', help='å†…å®¹CSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    parser.add_argument('stock_name', nargs='?', default='ç›®æ ‡è‚¡ç¥¨', help='è‚¡ç¥¨åç§°')
    parser.add_argument('--no-finbert', action='store_true', help='ç¦ç”¨ FinBERTï¼Œä»…ä½¿ç”¨å…³é”®è¯åŒ¹é…')

    args = parser.parse_args()

    # è§£æè‚¡ç¥¨å
    stock_name = args.stock_name if args.stock_name != 'ç›®æ ‡è‚¡ç¥¨' else 'è‚¡ç¥¨åˆ†æ'

    # ç¡®å®šæ˜¯å¦ç”Ÿæˆä¸¤ä»½æŠ¥å‘Šï¼ˆå…³é”®è¯ + FinBERTï¼‰
    if args.no_finbert:
        # ç”¨æˆ·æŒ‡å®š --no-finbertï¼Œåªç”Ÿæˆå…³é”®è¯æŠ¥å‘Š
        generate_both = False
    else:
        # è‡ªåŠ¨æ¨¡å¼æˆ–æ‰‹åŠ¨æ¨¡å¼éƒ½ç”Ÿæˆä¸¤ä»½æŠ¥å‘Š
        generate_both = True

    # æ•è·æ§åˆ¶å°è¾“å‡º
    old_stdout = sys.stdout

    try:
        # è‡ªåŠ¨æ¨¡å¼æˆ–æ‰‹åŠ¨æ¨¡å¼éƒ½éœ€è¦å…ˆè·å–æ–‡ä»¶è·¯å¾„
        if args.auto:
            sys.stdout = old_stdout  # ä¸´æ—¶æ¢å¤ï¼Œæ‰“å°æŸ¥æ‰¾ä¿¡æ¯
            print(f"\nğŸ” è‡ªåŠ¨æ¨¡å¼ï¼šæŸ¥æ‰¾æœ€æ–°å»é‡æ–‡ä»¶...")
            print(f"   æ•°æ®ç›®å½•: {args.data_dir}")

            comments_file, contents_file = find_latest_dedup_files(args.data_dir)

            # å¦‚æœæ‰¾ä¸åˆ°å»é‡æ–‡ä»¶ï¼Œè‡ªåŠ¨è¿è¡Œå»é‡è„šæœ¬
            if not comments_file:
                print("\nâš ï¸  æœªæ‰¾åˆ°å»é‡æ–‡ä»¶ï¼Œè‡ªåŠ¨è¿è¡Œå»é‡è„šæœ¬...")
                print("="*80)
                import subprocess
                import locale
                script_dir = Path(__file__).parent
                dedup_script = script_dir / "stock_sentiment_dedup.py"
                # ä½¿ç”¨ç³»ç»Ÿé»˜è®¤ç¼–ç ï¼Œé¿å… Windows GBK ç¼–ç é—®é¢˜
                result = subprocess.run(
                    ["uv", "run", "python", str(dedup_script), "--auto"],
                    encoding=locale.getpreferredencoding(),
                    errors='replace'
                )
                print("="*80)
                if result.returncode == 0:
                    print("âœ… å»é‡å®Œæˆï¼")
                    # é‡æ–°æŸ¥æ‰¾å»é‡æ–‡ä»¶
                    comments_file, contents_file = find_latest_dedup_files(args.data_dir)
                    if not comments_file:
                        print("\nâŒ å»é‡å¤±è´¥ï¼Œæ— æ³•ç»§ç»­åˆ†æ")
                        sys.exit(1)
                else:
                    print(f"\nâŒ å»é‡è„šæœ¬æ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                    sys.exit(1)

            print(f"   âœ“ è¯„è®ºæ–‡ä»¶: {comments_file.name}")
            if contents_file:
                print(f"   âœ“ å†…å®¹æ–‡ä»¶: {contents_file.name}")

            comments_path = str(comments_file)
            contents_path = str(contents_file) if contents_file else None
        else:
            # æ‰‹åŠ¨æ¨¡å¼
            if not args.comments_file:
                parser.error("è¯·æŒ‡å®š --auto è‡ªåŠ¨æ¨¡å¼ï¼Œæˆ–æä¾› comments_file è·¯å¾„")

            comments_path = args.comments_file
            contents_path = args.contents_file

        # ========================================================================
        # ç¬¬ä¸€ä»½æŠ¥å‘Šï¼šå…³é”®è¯åˆ†æï¼ˆä¸ä½¿ç”¨ FinBERTï¼‰
        # ========================================================================
        print("\n" + "="*80)
        print("ğŸ“Š ç”Ÿæˆç¬¬ 1/2 ä»½æŠ¥å‘Šï¼šå…³é”®è¯åˆ†æ")
        print("="*80)

        sys.stdout = mystdout_keyword = StringIO()

        analyze_stock_sentiment(
            comments_file=comments_path,
            contents_file=contents_path,
            stock_name=stock_name,
            use_finbert=False  # çº¯å…³é”®è¯
        )

        keyword_report = mystdout_keyword.getvalue()
        sys.stdout = old_stdout
        print(keyword_report)  # æ‰“å°åˆ°æ§åˆ¶å°

        save_report_to_file(keyword_report, stock_name, args.output_dir, suffix="_å…³é”®è¯")

        # ========================================================================
        # ç¬¬äºŒä»½æŠ¥å‘Šï¼šFinBERT åˆ†æ
        # ========================================================================
        if generate_both:
            print("\n" + "="*80)
            print("ğŸ¤– ç”Ÿæˆç¬¬ 2/2 ä»½æŠ¥å‘Šï¼šFinBERT åˆ†æ")
            print("="*80)

            sys.stdout = mystdout_finbert = StringIO()

            analyze_stock_sentiment(
                comments_file=comments_path,
                contents_file=contents_path,
                stock_name=stock_name,
                use_finbert=True  # FinBERT
            )

            finbert_report = mystdout_finbert.getvalue()
            sys.stdout = old_stdout
            print(finbert_report)  # æ‰“å°åˆ°æ§åˆ¶å°

            save_report_to_file(finbert_report, stock_name, args.output_dir, suffix="_FinBERT")

    except Exception as e:
        sys.stdout = old_stdout
        print(f"\nâŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
