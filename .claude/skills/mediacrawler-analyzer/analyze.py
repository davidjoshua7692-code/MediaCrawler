"""
MediaCrawler æ™ºèƒ½æ•°æ®åˆ†æå™¨
è‡ªåŠ¨è¯†åˆ«å¹³å°ç±»å‹å¹¶åŠ è½½ç›¸åº”çš„åˆ†æé…ç½®
"""
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import re
from collections import Counter
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

# ============================================================================
# å¹³å°åˆ†æé…ç½®åº“
# ============================================================================

PLATFORM_ANALYSIS_CONFIG = {
    'xiaohongshu': {
        'name': 'å°çº¢ä¹¦',
        'platform_keywords': ['note_id', 'xsec_token', 'collected_count'],
        'content_fields': ['title', 'desc', 'tag_list'],
        'metrics': ['liked_count', 'collected_count', 'comment_count', 'share_count'],
        'text_fields': ['title', 'desc'],
        'location_field': 'ip_location',
        'special_keywords': {
            'features': {
                'å®‰é™': ['å®‰é™', 'æ¸…å‡€', 'ä¸åµ', 'silent', 'quiet'],
                'æ’åº§': ['æ’åº§', 'ç”µæº', 'å……ç”µ', 'plug'],
                'ç½‘ç»œ': ['wifi', 'wi-fi', 'ç½‘é€Ÿ', 'ç½‘ç»œ'],
                'åœè½¦ä½': ['åœè½¦', 'parking', 'åœè½¦åˆ¸'],
                'æœ‰å•æ‰€': ['å•æ‰€', 'å«ç”Ÿé—´', 'æ´—æ‰‹é—´', 'wc'],
                'ä»·æ ¼': ['ä»·æ ¼', 'ä¾¿å®œ', 'è´µ', 'å®æƒ ', 'äººå‡'],
            },
            'sentiment': {
                'positive': ['æ¨è', 'å¥½', 'ä¸é”™', 'èˆ’æœ', 'æ£’', 'å–œæ¬¢', 'é€‚åˆ', 'æ–¹ä¾¿'],
                'negative': ['åµ', 'è´µ', 'å·®', 'ä¸å¥½', 'å¤±æœ›', 'æ…¢', 'æŒ¤']
            }
        }
    },
    'douyin': {
        'name': 'æŠ–éŸ³',
        'platform_keywords': ['aweme_id', 'sec_uid'],
        'content_fields': ['title', 'desc'],
        'metrics': ['liked_count', 'comment_count', 'share_count'],
        'text_fields': ['title', 'desc'],
        'location_field': 'ip_location',
        'special_keywords': {
            'features': {},
            'sentiment': {
                'positive': ['å¥½çœ‹', 'ä¸é”™', 'æ¨è', 'å–œæ¬¢', 'çˆ±äº†'],
                'negative': ['ä¸å¥½çœ‹', 'æ— èŠ', 'å·®']
            }
        }
    },
    'bilibili': {
        'name': 'Bç«™',
        'platform_keywords': ['bvid', 'video_play_count'],
        'content_fields': ['title', 'desc'],
        'metrics': ['liked_count', 'video_play_count', 'video_coin_count', 'video_collect_count'],
        'text_fields': ['title', 'desc'],
        'location_field': None,
        'special_keywords': {
            'features': {},
            'sentiment': {
                'positive': ['å¥½çœ‹', 'ä¸é”™', 'æ¨è', 'å¹²è´§', 'å®ç”¨'],
                'negative': ['æ°´', 'ä¸å¥½çœ‹', 'å·®']
            }
        }
    },
    'weibo': {
        'name': 'å¾®åš',
        'platform_keywords': ['mid', 'mblogid'],
        'content_fields': ['text', 'topic_list'],
        'metrics': ['liked_count', 'comments_count', 'reposts_count'],
        'text_fields': ['text'],
        'location_field': None,
        'special_keywords': {
            'features': {},
            'sentiment': {
                'positive': ['èµ', 'æ”¯æŒ', 'æ¨è'],
                'negative': ['åæ§½', 'å·®']
            }
        }
    }
}

# ============================================================================
# å¹³å°æ£€æµ‹å™¨
# ============================================================================

def detect_platform(contents_df, comments_df=None):
    """
    æ™ºèƒ½æ£€æµ‹å¹³å°ç±»å‹

    Args:
        contents_df: å†…å®¹DataFrame
        comments_df: è¯„è®ºDataFrameï¼ˆå¯é€‰ï¼‰

    Returns:
        str: å¹³å°æ ‡è¯†ç¬¦
    """
    columns = set(contents_df.columns.tolist())

    # é€šè¿‡ç‰¹å¾åˆ—åè¯†åˆ«å¹³å°
    for platform_id, config in PLATFORM_ANALYSIS_CONFIG.items():
        if any(keyword in columns for keyword in config['platform_keywords']):
            return platform_id

    # å¦‚æœæ— æ³•è¯†åˆ«ï¼Œå°è¯•ä»æ–‡ä»¶è·¯å¾„æ¨æ–­
    # ï¼ˆè°ƒç”¨æ–¹éœ€è¦ä¼ å…¥æ–‡ä»¶è·¯å¾„å‚æ•°ï¼‰

    return 'generic'

def extract_locations(text, platform='xiaohongshu'):
    """
    æå–åœ°ç†ä½ç½®ä¿¡æ¯

    Args:
        text: æ–‡æœ¬å†…å®¹
        platform: å¹³å°ç±»å‹

    Returns:
        list: æå–åˆ°çš„åœ°ç‚¹åˆ—è¡¨
    """
    locations = []

    if pd.isna(text):
        return locations

    text_str = str(text)

    # é€šç”¨åœ°ç‚¹æ¨¡å¼
    location_patterns = [
        r'(\w+è·¯)', r'(\w+å¹¿åœº)', r'(\w+å•†åœº)', r'(\w+è´­ç‰©ä¸­å¿ƒ)',
        r'(\w+å¤§å­¦)', r'(\w+å…¬å›­)', r'å›¾ä¹¦é¦†', r'åœ°é“ç«™'
    ]

    # å¹³å°ç‰¹å®šåœ°ç‚¹
    if platform == 'xiaohongshu':
        shanghai_districts = [
            'å¾æ±‡', 'é™å®‰', 'é»„æµ¦', 'é•¿å®', 'æ™®é™€', 'è™¹å£',
            'æ¨æµ¦', 'æµ¦ä¸œ', 'é—µè¡Œ', 'å®å±±', 'å˜‰å®š', 'æ¾æ±Ÿ',
            'é’æµ¦', 'å¥‰è´¤', 'é‡‘å±±', 'å´‡æ˜'
        ]
        location_patterns.insert(0, r'(' + '|'.join(shanghai_districts) + r')')

    for pattern in location_patterns:
        matches = re.findall(pattern, text_str)
        locations.extend(matches)

    return locations

def analyze_features(text, platform='xiaohongshu', custom_keywords=None):
    """
    åˆ†ææ–‡æœ¬ç‰¹å¾

    Args:
        text: æ–‡æœ¬å†…å®¹
        platform: å¹³å°ç±»å‹
        custom_keywords: è‡ªå®šä¹‰å…³é”®è¯å­—å…¸ï¼ˆå¯é€‰ï¼‰

    Returns:
        list: æå–åˆ°çš„ç‰¹å¾åˆ—è¡¨
    """
    features = []

    if pd.isna(text):
        return features

    text_lower = str(text).lower()

    # ä½¿ç”¨è‡ªå®šä¹‰å…³é”®è¯æˆ–å¹³å°é»˜è®¤å…³é”®è¯
    if custom_keywords and 'features' in custom_keywords:
        feature_keywords = custom_keywords['features']
    else:
        feature_keywords = PLATFORM_ANALYSIS_CONFIG.get(
            platform, {}
        ).get('special_keywords', {}).get('features', {})

    for feature, keywords in feature_keywords.items():
        for keyword in keywords:
            if keyword.lower() in text_lower:
                features.append(feature)
                break

    return features

def analyze_sentiment(text, platform='xiaohongshu', custom_keywords=None):
    """
    åˆ†ææƒ…æ„Ÿå€¾å‘

    Args:
        text: æ–‡æœ¬å†…å®¹
        platform: å¹³å°ç±»å‹
        custom_keywords: è‡ªå®šä¹‰å…³é”®è¯å­—å…¸ï¼ˆå¯é€‰ï¼‰

    Returns:
        str: 'positive', 'negative', æˆ– 'neutral'
    """
    if pd.isna(text):
        return 'neutral'

    text_lower = str(text).lower()

    # ä½¿ç”¨è‡ªå®šä¹‰å…³é”®è¯æˆ–å¹³å°é»˜è®¤å…³é”®è¯
    if custom_keywords and 'sentiment' in custom_keywords:
        sentiment_keywords = custom_keywords['sentiment']
    else:
        sentiment_keywords = PLATFORM_ANALYSIS_CONFIG.get(
            platform, {}
        ).get('special_keywords', {}).get('sentiment', {})

    positive_words = sentiment_keywords.get('positive', [])
    negative_words = sentiment_keywords.get('negative', [])

    for word in positive_words:
        if word.lower() in text_lower:
            return 'positive'

    for word in negative_words:
        if word.lower() in text_lower:
            return 'negative'

    return 'neutral'

# ============================================================================
# ä¸»åˆ†æå‡½æ•°
# ============================================================================

def analyze_mediacrawler_data(
    contents_file,
    comments_file=None,
    custom_keywords=None,
    custom_title=None
):
    """
    ç»¼åˆåˆ†æMediaCrawlerçˆ¬å–çš„æ•°æ®

    Args:
        contents_file: å†…å®¹CSVæ–‡ä»¶è·¯å¾„
        comments_file: è¯„è®ºCSVæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        custom_keywords: è‡ªå®šä¹‰å…³é”®è¯å­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                'features': {
                    'ç‰¹å¾å': ['å…³é”®è¯1', 'å…³é”®è¯2'],
                    ...
                },
                'sentiment': {
                    'positive': ['å¥½è¯1', 'å¥½è¯2'],
                    'negative': ['åè¯1', 'åè¯2']
                }
            }
        custom_title: è‡ªå®šä¹‰åˆ†ææŠ¥å‘Šæ ‡é¢˜

    Returns:
        dict: åˆ†æç»“æœå’Œå¯è§†åŒ–æ–‡ä»¶è·¯å¾„
    """
    # è¯»å–æ•°æ®
    df_contents = pd.read_csv(contents_file)
    df_comments = pd.read_csv(comments_file) if comments_file else None

    # æ™ºèƒ½æ£€æµ‹å¹³å°
    platform = detect_platform(df_contents, df_comments)
    platform_config = PLATFORM_ANALYSIS_CONFIG.get(platform, {})
    platform_name = platform_config.get('name', platform.title())

    # è‡ªå®šä¹‰æ ‡é¢˜
    if not custom_title:
        custom_title = f"ğŸ“Š {platform_name}æ•°æ®åˆ†ææŠ¥å‘Š"

    print("=" * 80)
    print(custom_title)
    print("=" * 80)

    # æ•°æ®æ¦‚è§ˆ
    print(f"\nâœ… å¹³å°è¯†åˆ«: {platform_name} ({platform})")
    print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ!")
    print(f"   å¸–å­æ•°æ®: {len(df_contents)} æ¡")
    if df_comments is not None:
        print(f"   è¯„è®ºæ•°æ®: {len(df_comments)} æ¡")

    # 1. åŸºç¡€ç»Ÿè®¡
    print("\n" + "=" * 80)
    print("ğŸ“ˆ ä¸€ã€åŸºç¡€æ•°æ®ç»Ÿè®¡")
    print("=" * 80)

    metrics = platform_config.get('metrics', [])
    metric_names = [m for m in metrics if m in df_contents.columns]

    if metric_names:
        print(f"\näº’åŠ¨æ•°æ®ç»Ÿè®¡:")
        for metric in metric_names:
            mean_val = df_contents[metric].mean()
            max_val = df_contents[metric].max()
            metric_label = metric.replace('_', ' ').title()
            print(f"  å¹³å‡{metric_label}: {mean_val:.1f}")
            print(f"  æœ€é«˜{metric_label}: {max_val}")

    # 2. åœ°ç†ä½ç½®åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ“ äºŒã€åœ°ç†ä½ç½®åˆ†æ")
    print("=" * 80)

    location_field = platform_config.get('location_field')
    all_locations = []

    if location_field and location_field in df_contents.columns:
        # ä»ä¸“ç”¨å­—æ®µæå–
        locations_data = df_contents[location_field].value_counts().head(10)
        print(f"\n{location_field} åˆ†å¸ƒ Top 10:")
        for loc, count in locations_data.items():
            if pd.notna(loc):
                print(f"  {loc}: {count} æ¬¡")
    else:
        # ä»æ–‡æœ¬ä¸­æå–
        text_fields = platform_config.get('text_fields', [])
        for idx, row in df_contents.iterrows():
            text = ' '.join([str(row.get(field, '')) for field in text_fields])
            locations = extract_locations(text, platform)
            all_locations.extend(locations)

        location_counter = Counter(all_locations)
        if location_counter:
            print(f"\næ–‡æœ¬ä¸­æåŠçš„åœ°ç‚¹ Top 10:")
            for location, count in location_counter.most_common(10):
                print(f"  {location}: {count} æ¬¡")

    # 3. ç‰¹å¾åˆ†æ
    print("\n" + "=" * 80)
    print("ğŸ¯ ä¸‰ã€å†…å®¹ç‰¹å¾åˆ†æ")
    print("=" * 80)

    all_features = []
    text_fields = platform_config.get('text_fields', [])

    for idx, row in df_contents.iterrows():
        text = ' '.join([str(row.get(field, '')) for field in text_fields])
        features = analyze_features(text, platform, custom_keywords)
        all_features.extend(features)

    feature_counter = Counter(all_features)

    if feature_counter:
        print(f"\nç‰¹å¾æåŠæ¬¡æ•° Top 10:")
        for feature, count in feature_counter.most_common(10):
            print(f"  {feature}: {count} æ¬¡")
    else:
        print("\næœªæ£€æµ‹åˆ°æ˜¾è‘—ç‰¹å¾ï¼ˆå¯ä½¿ç”¨custom_keywordså‚æ•°è‡ªå®šä¹‰ç‰¹å¾åº“ï¼‰")

    # 4. æƒ…æ„Ÿåˆ†æ
    if df_comments is not None:
        print("\n" + "=" * 80)
        print("ğŸ’¬ å››ã€è¯„è®ºæƒ…æ„Ÿåˆ†æ")
        print("=" * 80)

        sentiment_results = {'positive': 0, 'negative': 0, 'neutral': 0}

        for idx, row in df_comments.head(200).iterrows():
            comment = row.get('content', '')
            sentiment = analyze_sentiment(comment, platform, custom_keywords)
            sentiment_results[sentiment] += 1

        total = sentiment_results['positive'] + sentiment_results['negative']
        positive_pct = (sentiment_results['positive'] / total * 100) if total > 0 else 0

        print(f"\nè¯„è®ºæƒ…æ„Ÿåˆ†å¸ƒ (åŸºäºå‰200æ¡è¯„è®º):")
        print(f"  ç§¯æ: {sentiment_results['positive']} æ¡")
        print(f"  æ¶ˆæ: {sentiment_results['negative']} æ¡")
        print(f"  ä¸­æ€§: {sentiment_results['neutral']} æ¡")
        if total > 0:
            print(f"  ç§¯æå æ¯”: {positive_pct:.1f}%")

    # 5. åˆ›å»ºå¯è§†åŒ–
    print("\n" + "=" * 80)
    print("ğŸ“Š äº”ã€ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print("=" * 80)

    output_file = create_visualizations(
        df_contents,
        df_comments,
        platform,
        platform_config,
        location_counter if all_locations else None,
        feature_counter
    )

    print(f"\nâœ… å›¾è¡¨å·²ä¿å­˜: {output_file}")

    # 6. çƒ­é—¨å†…å®¹
    print("\n" + "=" * 80)
    print("ğŸ”¥ å…­ã€çƒ­é—¨å†…å®¹ Top 3")
    print("=" * 80)

    if metric_names:
        primary_metric = metric_names[0]
        top_contents = df_contents.nlargest(3, primary_metric)

        for idx, row in top_contents.iterrows():
            title = row.get('title', row.get('text', 'N/A'))
            print(f"\n  {str(title)[:60]}...")
            print(f"  ğŸ‘ {row[primary_metric]} {primary_metric}")

    # 7. å…³é”®æ´å¯Ÿ
    print("\n" + "=" * 80)
    print("ğŸ’¡ ä¸ƒã€å…³é”®æ´å¯Ÿ")
    print("=" * 80)

    insights = []

    if feature_counter:
        top_feature = feature_counter.most_common(1)[0]
        insights.append(f"ç”¨æˆ·æœ€å…³æ³¨: {top_feature[0]} (æåŠ{top_feature[1]}æ¬¡)")

    if all_locations:
        top_location = location_counter.most_common(1)[0]
        insights.append(f"æœ€çƒ­é—¨åŒºåŸŸ: {top_location[0]} (æåŠ{top_location[1]}æ¬¡)")

    if df_comments is not None:
        insights.append(f"è¯„è®ºæƒ…æ„Ÿå€¾å‘: ç§¯æ{positive_pct:.1f}%")

    if metric_names and len(metric_names) >= 2:
        metric1_mean = df_contents[metric_names[0]].mean()
        metric2_mean = df_contents[metric_names[1]].mean()
        ratio = metric2_mean / metric1_mean if metric1_mean > 0 else 0
        insights.append(f"{metric_names[1]}æ˜¯{metric_names[0]}çš„{ratio:.2f}å€")

    for insight in insights:
        print(f"  â€¢ {insight}")

    print("\n" + "=" * 80)
    print("âœ… åˆ†æå®Œæˆ!")
    print("=" * 80)

    return {
        'platform': platform,
        'contents_count': len(df_contents),
        'comments_count': len(df_comments) if df_comments is not None else 0,
        'top_features': feature_counter.most_common(5),
        'top_locations': location_counter.most_common(5) if all_locations else [],
        'visualization': output_file
    }

def create_visualizations(
    df_contents,
    df_comments,
    platform,
    platform_config,
    location_counter,
    feature_counter
):
    """åˆ›å»ºç»¼åˆå¯è§†åŒ–å›¾è¡¨"""

    fig = plt.figure(figsize=(16, 12))
    metrics = platform_config.get('metrics', [])
    available_metrics = [m for m in metrics if m in df_contents.columns]

    # å›¾1: äº’åŠ¨æ•°æ®åˆ†å¸ƒ
    ax1 = plt.subplot(2, 3, 1)
    if available_metrics:
        for metric in available_metrics[:3]:
            data = df_contents[metric].dropna()
            if len(data) > 0:
                plt.hist(data, bins=20, alpha=0.5, label=metric.replace('_', ' ').title())
        plt.xlabel('æ•°é‡')
        plt.ylabel('å¸–å­æ•°')
        plt.title('äº’åŠ¨æ•°æ®åˆ†å¸ƒ')
        plt.legend()
        plt.grid(True, alpha=0.3)

    # å›¾2: äº’åŠ¨æ•°æ®ç›¸å…³æ€§
    ax2 = plt.subplot(2, 3, 2)
    if len(available_metrics) > 1:
        corr_data = df_contents[available_metrics].dropna()
        if len(corr_data) > 0:
            corr = corr_data.corr()
            sns.heatmap(corr, annot=True, cmap='coolwarm', center=0,
                       square=True, linewidths=1, ax=ax2, cbar_kws={'shrink': 0.8})
            ax2.set_title('äº’åŠ¨æ•°æ®ç›¸å…³æ€§')

    # å›¾3: çƒ­é—¨åŒºåŸŸ/åœ°ç‚¹
    ax3 = plt.subplot(2, 3, 3)
    if location_counter:
        top_locations = dict(location_counter.most_common(10))
        plt.barh(range(len(top_locations)), list(top_locations.values()))
        plt.yticks(range(len(top_locations)), list(top_locations.keys()))
        plt.xlabel('æåŠæ¬¡æ•°')
        plt.title('çƒ­é—¨åœ°ç‚¹ Top 10')
        plt.grid(True, alpha=0.3, axis='x')

    # å›¾4: å†…å®¹ç‰¹å¾æ’å
    ax4 = plt.subplot(2, 3, 4)
    if feature_counter:
        top_features = dict(feature_counter.most_common(10))
        plt.barh(range(len(top_features)), list(top_features.values()))
        plt.yticks(range(len(top_features)), list(top_features.keys()))
        plt.xlabel('æåŠæ¬¡æ•°')
        plt.title('å†…å®¹ç‰¹å¾ Top 10')
        plt.grid(True, alpha=0.3, axis='x')

    # å›¾5: IPåœ°ç‚¹åˆ†å¸ƒï¼ˆå¦‚æœæœ‰ï¼‰
    ax5 = plt.subplot(2, 3, 5)
    location_field = platform_config.get('location_field')
    if location_field and location_field in df_contents.columns:
        ip_locations = df_contents[location_field].value_counts().head(10)
        if len(ip_locations) > 0:
            plt.barh(range(len(ip_locations)), ip_locations.values)
            plt.yticks(range(len(ip_locations)), ip_locations.index)
            plt.xlabel('å¸–å­æ•°')
            plt.title(f'{location_field} åˆ†å¸ƒ')
            plt.grid(True, alpha=0.3, axis='x')

    # å›¾6: æ•°æ®è´¨é‡
    ax6 = plt.subplot(2, 3, 6)
    missing_data = df_contents.isnull().sum()
    missing_data = missing_data[missing_data > 0].sort_values(ascending=True)
    if len(missing_data) > 0:
        plt.barh(range(len(missing_data)), missing_data.values)
        plt.yticks(range(len(missing_data)), missing_data.index)
        plt.xlabel('ç¼ºå¤±æ•°é‡')
        plt.title('æ•°æ®ç¼ºå¤±æƒ…å†µ')
        plt.grid(True, alpha=0.3, axis='x')
    else:
        ax6.text(0.5, 0.5, 'æ•°æ®å®Œæ•´\næ— ç¼ºå¤±', ha='center', va='center',
                fontsize=14, transform=ax6.transAxes)
        ax6.set_title('æ•°æ®è´¨é‡')

    plt.tight_layout()

    output_file = f'd:/MediaCrawler-main/{platform}_analysis.png'
    plt.savefig(output_file, dpi=150, bbox_inches='tight')
    plt.close()

    return output_file

# ============================================================================
# å‘½ä»¤è¡Œæ¥å£
# ============================================================================

if __name__ == "__main__":
    import sys

    # ç®€å•çš„å‘½ä»¤è¡Œæ¥å£
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python analyze.py <contents.csv> [comments.csv]")
        sys.exit(1)

    contents_file = sys.argv[1]
    comments_file = sys.argv[2] if len(sys.argv) > 2 else None

    results = analyze_mediacrawler_data(contents_file, comments_file)
