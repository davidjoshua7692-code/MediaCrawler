# MediaCrawler AI 智能体工作流

## 🚀 核心工作流

```
用户发起爬虫需求
    ↓
AI智能推理：解析关键词 + 完善参数配置
    ↓
与用户互动确认
    ↓
执行爬虫自动化脚本
    ↓
⚠️ 使用 `analyzing-social-media-data` skill 进行数据分析 
    ↓
结果解读，转化为用户洞察
```

---

## 🤖 AI 行为准则（核心原则）

### 1️⃣ 关键词智能推理（必须优化，不能机械执行）

**❌ 错误示例：**
```python
# 用户输入：上海市适合移动办公的咖啡厅/空间
KEYWORDS = "上海市适合移动办公的咖啡厅/空间"  # 只能搜到很少结果
```

**✅ 正确示例：**
```python
# AI推理：拆分复合概念 + 提取核心概念 + 扩展同义词 + 简化冗余
KEYWORDS = "上海办公咖啡厅,上海移动办公空间,上海适合工作咖啡馆,上海共享办公,上海自习室"
```

**优化策略**：
- 去掉主观词（"最好"、"最火"）
- 拆分复合概念（地点+用途+场所类型）
- 扩展同义词和相关术语
- 简化地名（"上海市"→"上海"）
- 始终与用户互动确认优化后的关键词组合

### 2️⃣ 参数智能补全（必须与用户确认）

**爬取数量推理**：
| 任务类型 | 建议数量 |
|----------|----------|
| 初步探索 | 10-20 条 |
| 竞品分析 | 30-50 条 |
| 深度研究 | 50-100 条 |

**保存格式推理**：
| 用户需求场景 | 推荐格式 |
|-------------|----------|
| 数据分析 | Excel |
| Python处理 | JSON/CSV |
| 非技术人员分享 | Excel |
| 长期存储大量数据 | SQLite |

**是否爬评论推理**：
- 分析用户反馈/热门话题 → ✅ 必须
- 只收集标题和图片 → ❌ 不需要

**平台选择推理**：
| 内容类型 | 推荐平台 |
|----------|----------|
| 生活方式/美食/穿搭 | 小红书（xhs） |
| 短视频/音乐/舞蹈 | 抖音（dy） |
| 教程/知识/长视频 | B站（bili） |
| 热点新闻/话题讨论 | 微博（wb） |
| 社区讨论/兴趣圈 | 贴吧（tieba） |
| 问答/专业讨论 | 知乎（zhihu） |
| 短视频/直播 | 快手（ks） |

### 3️⃣ 执行前必须检查

```
✓ 关键词是否已优化？
✓ 爬取数量是否合理？
✓ 保存格式是否适合用户需求？
✓ 是否需要爬评论？
✓ 平台选择是否正确？
✓ Excel格式是否已安装openpyxl？
✓ 抖音/知乎是否已安装Node.js？
```

---

## 🎯 用户交互规则

### 必须询问的场景
1. **关键词模糊** → 推荐优化后的关键词组合
2. **未指定数量** → 根据用途推荐数量
3. **未指定格式** → 推荐Excel（便于直接查看）
4. **未指定平台** → 根据内容类型推荐平台
5. **特殊依赖** → 确认环境（Node.js、openpyxl）

### 确认对话模板
```
我理解你想爬取「[原关键词]」。

推荐优化为：「[优化后关键词]」
原因：[优化理由]

其他参数建议：
• 爬取数量：[N]条（适合[场景]）
• 保存格式：[格式]（便于[用途]）
• 爬取评论：[是/否]（因为[理由]）

是否使用以上配置？
```

---

## 🔧 执行方法

### 1. 修改配置文件
编辑 `config/base_config.py`：

```python
# 第21行 - 平台
PLATFORM = "xhs"  # xhs/dy/bili/ks/wb/tieba/zhihu

# 第22行 - 关键词（AI智能优化后）
KEYWORDS = "优化后的关键词组"

# 第26行 - 爬取类型
CRAWLER_TYPE = "search"  # search/detail/creator

# 第83行 - 爬取数量
CRAWLER_MAX_NOTES_COUNT = 20

# 第92行 - 是否爬评论
ENABLE_GET_COMMENTS = True

# 第74行 - 保存格式
SAVE_DATA_OPTION = "json"  # json/csv/db/sqlite/excel/postgres
```

### 2. 运行爬虫
```bash
uv run python main.py
```

### 3. 数据位置
- JSON格式：`data/{平台}/json/`
- CSV格式：`data/{平台}/csv/`
- Excel格式：`data/{平台}/excel/`

---

## ⚠️ 重要提示：数据分析流程

**当爬虫完成后，如果保存格式为CSV或Excel，AI必须继续使用 `analyzing-social-media-data` skill 进行数据分析。**

---

## 💡 结果解读与洞察转化（行为准则）

### 核心原则：不只是输出数据，要提供洞察

**❌ 错误示例：**
```
分析完成。共找到100条数据。
高频词：咖啡（50次）、安静（30次）、插座（25次）
```

**✅ 正确示例：**
```
根据对100条小红书笔记的分析，发现用户选择办公咖啡厅时最关注以下特征：

🏆 **核心需求**（出现频率>50%）
• 安静环境（85%提及）- 这是首要条件
• 插座充足（70%）- 说明长时间办公是常见场景
• WiFi稳定（65%）- 网络是基础设施

💰 **价格容忍度**
• 人均30-50元被认为合理（60%用户接受）
• 超过70元会被标记为"贵"（40%负面评价）

📍 **热门区域**
上海：静安寺、徐家汇、五角场（商圈周边）

💡 **关键洞察**
用户更看重"工作体验"而非"咖啡品质"，环境安静度比咖啡口味重要3倍。
```

### 必须包含的元素

1. **数据背景** - 说明分析了什么数据（平台、关键词、数量）
2. **核心发现** - 用百分比、排名等量化指标支撑
3. **分层呈现** - 按重要性分组（高频、中频、低频）
4. **可操作洞察** - 告诉用户"这意味着什么"
5. **可视化表达** - 使用emoji、表格、对比等增强可读性

### 洞察转化模板

#### 模板1：特征分析类
```
根据 [N] 条 [平台] 关于「[关键词]」的数据分析：

🔥 **最受关注的特征**（提及率>50%）
• [特征1]（[X]%） - [洞察说明]
• [特征2]（[X]%） - [洞察说明]

⭐ **次要但重要的特征**（提及率20-50%）
• [特征3]（[X]%） - [洞察说明]

💡 **关键洞察**
[总结性发现，对比分析，趋势判断]
```

#### 模板2：情感分析类
```
根据 [N] 条评论分析：

😊 **正面评价**（[X]%）
主要集中在：[维度1]、[维度2]、[维度3]

😞 **负面评价**（[X]%）
主要问题：[问题1]（[X]条）、[问题2]（[X]条）

💡 **关键洞察**
[用户满意度判断，改进建议]
```

#### 模板3：对比分析类
```
数据对比发现：

📊 **维度对比**
| 维度 | 高分内容特征 | 低分内容特征 |
|------|------------|-------------|
| [维度1] | [特征] | [特征] |
| [维度2] | [特征] | [特征] |

💡 **关键洞察**
[成功因素 vs 失败因素]
```

### 禁止行为

❌ 只输出技术统计数据，不解释含义
❌ 使用专业术语，不考虑用户理解
❌ 忽略数据背后的"为什么"
❌ 不提供可操作的建议或判断

---

## 📚 项目特殊性和约束

### 项目独特性
1. **多平台支持**：支持7个平台（小红书/抖音/B站/快手/微博/贴吧/知乎）
2. **CDP模式**：默认使用真实Chrome浏览器（反检测）
3. **关键词分隔符**：必须用英文逗号分隔多个关键词
4. **配置中心化**：所有参数集中在 `config/base_config.py`

### 特殊依赖约束
- **Excel格式** → 需要 `openpyxl`（运行 `uv sync`）
- **抖音/知乎** → 需要 Node.js >= v16.0.0
- **数据库** → 首次使用必须运行 `uv run main.py --init_db sqlite`

### 执行约束
- **登录方式**：首次运行需扫码登录
- **并发限制**：`MAX_CONCURRENCY_NUM` 不超过5
- **请求间隔**：程序自动控制（避免被封）

---

## 📋 关键文件索引

### 核心配置
- [config/base_config.py](file:///D:/MediaCrawler-main/config/base_config.py) - 所有基础参数（必看）
- [main.py](file:///D:/MediaCrawler-main/main.py) - 程序入口

### 各平台配置
- [xhs_config.py](file:///D:/MediaCrawler-main/config/xhs_config.py) - 小红书
- [dy_config.py](file:///D:/MediaCrawler-main/config/dy_config.py) - 抖音
- [bili_config.py](file:///D:/MediaCrawler-main/config/bilibili_config.py) - B站
- [wb_config.py](file:///D:/MediaCrawler-main/config/weibo_config.py) - 微博

### 数据分析
- [.claude/skills/analyzing-social-media-data/SKILL.md](file:///D:/MediaCrawler-main/.claude/skills/analyzing-social-media-data/SKILL.md) - **数据分析工作流（强制使用）**

---

## 完整参数表（按需查询）

| 行号 | 参数名 | 默认值 | 常用值 |
|-----|-------|--------|--------|
| 21  | PLATFORM | xhs | xhs/dy/bili/ks/wb/tieba/zhihu |
| 22  | KEYWORDS | 编程副业,编程兼职 | 任意关键词，逗号分隔 |
| 26  | CRAWLER_TYPE | search | search/detail/creator |
| 74  | SAVE_DATA_OPTION | json | json/csv/excel/sqlite/db/postgres |
| 83  | CRAWLER_MAX_NOTES_COUNT | 15 | 1-1000 |
| 92  | ENABLE_GET_COMMENTS | True | True/False |

---

## 典型使用示例

### 示例1：用户输入模糊
```
用户："帮我在小红书找适合办公的咖啡店"

AI推理：
1. 关键词优化："上海办公咖啡厅,移动办公空间,自习工作咖啡馆"
2. 数量推荐：30条（适合竞品分析）
3. 格式推荐：Excel（便于筛选）
4. 爬取评论：是（了解真实体验）

AI回复："我理解你想找适合办公的咖啡店。
推荐优化关键词为：「上海办公咖啡厅,移动办公空间,自习工作咖啡馆」
爬取30条数据，保存为Excel格式，包含用户评论。
是否使用这个配置？"
```

### 示例2：完成爬虫后
```
AI自动执行：
1. 检测到 CSV 文件：data/xhs/csv/search_contents_2026-01-19.csv
2. 加载 analyzing-social-media-data skill
3. 预览数据结构
4. 推荐分析模板（基于关键词"办公咖啡厅" → workspace模板）
5. 与用户确认分析维度
6. 执行分析
7. 转化为洞察："最受欢迎的特征是：安静（85%提及）、插座充足（70%）、咖啡好喝（60%）"
```
